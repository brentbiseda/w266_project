{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Hugging Face Transformers\n",
    "\n",
    "https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install keras\n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import transformers as ppb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-cased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/NER/train.tsv\", sep=\"\\t\", header=None)\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "test_df = pd.read_csv(\"../datasets/NER/test.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = test_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long sentences to 128 tokens\n",
    "X = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y = np.array(df[1])\n",
    "y = ['missing' if x is np.nan else x for x in y]\n",
    "del df\n",
    "\n",
    "X_test = test_df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y_test = np.array(test_df[1])\n",
    "y_test = ['missing' if x is np.nan else x for x in y_test]\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding of y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "y = encoder.transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# One hot Encoding of y test\n",
    "y_oh = encoder.transform(y_test)\n",
    "y_oh = to_categorical(y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(tokenizedBatch):\n",
    "    max_len = 0\n",
    "    for i in tokenizedBatch.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenizedBatch.values])\n",
    "    \n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    input_ids = torch.tensor(padded).to(torch.long)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for Batch: 1 of 4\n",
      "Generating Embeddings for Batch: 2 of 4\n",
      "Generating Embeddings for Batch: 3 of 4\n",
      "Generating Embeddings for Batch: 4 of 4\n",
      "Generating Test Embeddings for Batch: 1 of 2\n",
      "Generating Test Embeddings for Batch: 2 of 2\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000\n",
    "all_embeddings = []\n",
    "all_embeddings_test = []\n",
    "\n",
    "# Process Training Set Embeddings\n",
    "batches = math.ceil(X.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings.append(batchEmbeddings)\n",
    "\n",
    "# Process Test Set Embeddings\n",
    "batches = math.ceil(X_test.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Test Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X_test[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings_test.append(batchEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "all_embeddings_test = np.concatenate(all_embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../binary/bert_NER_embeddings_twitter.npy', all_embeddings)\n",
    "np.save('../binary/y_NER_twitter.npy', y)\n",
    "np.save('../binary/bert_NER_embeddings_test_twitter.npy', all_embeddings_test)\n",
    "np.save('../binary/y_test_NER_twitter.npy', y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.optimizers import adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('../binary/bert_NER_embeddings_twitter.npy')\n",
    "y = np.load('../binary/y_NER_twitter.npy')\n",
    "all_embeddings_test = np.load('../binary/bert_NER_embeddings_test_twitter.npy')\n",
    "y_oh = np.load('../binary/y_test_NER_twitter.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisedab\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#sgd = sgd(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optim = adam(lr=0.0003, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    embedding = Input(shape=(768,), dtype=\"float\")\n",
    "    dense1 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)\n",
    "    dense2 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense1)\n",
    "    dense3 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense2)\n",
    "    dense4 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense3)\n",
    "    dense5 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense4)\n",
    "    dense6 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense5)\n",
    "    dense7 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense6)\n",
    "    dense8 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense7)\n",
    "    dense9 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense8)\n",
    "    dense10 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense9)\n",
    "    pred = Dense(4, activation='sigmoid')(dense9)\n",
    "    model = Model(inputs=[embedding], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'], )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              769000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 8,781,004\n",
      "Trainable params: 8,781,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=15)\n",
    "cb_list = [es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisedab\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15619 samples, validate on 16 samples\n",
      "Epoch 1/1000\n",
      "15619/15619 [==============================] - 3s 188us/step - loss: 10.1096 - acc: 0.9364 - val_loss: 9.7668 - val_acc: 0.6250\n",
      "Epoch 2/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 9.2978 - acc: 0.9365 - val_loss: 10.3206 - val_acc: 0.6250\n",
      "Epoch 3/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 8.8093 - acc: 0.9365 - val_loss: 10.8356 - val_acc: 0.6250\n",
      "Epoch 4/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 8.6321 - acc: 0.9365 - val_loss: 9.4784 - val_acc: 0.6250\n",
      "Epoch 5/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 8.3554 - acc: 0.9365 - val_loss: 8.8783 - val_acc: 0.6250\n",
      "Epoch 6/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 8.1641 - acc: 0.9365 - val_loss: 8.9178 - val_acc: 0.6250\n",
      "Epoch 7/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.9306 - acc: 0.9365 - val_loss: 9.0735 - val_acc: 0.6250\n",
      "Epoch 8/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.7437 - acc: 0.9365 - val_loss: 8.8238 - val_acc: 0.6250\n",
      "Epoch 9/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.5435 - acc: 0.9365 - val_loss: 8.4129 - val_acc: 0.6250\n",
      "Epoch 10/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.3557 - acc: 0.9365 - val_loss: 8.1452 - val_acc: 0.6250\n",
      "Epoch 11/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.1677 - acc: 0.9365 - val_loss: 8.0403 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.9852 - acc: 0.9365 - val_loss: 7.9782 - val_acc: 0.6250\n",
      "Epoch 13/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.8149 - acc: 0.9365 - val_loss: 7.7958 - val_acc: 0.6250\n",
      "Epoch 14/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.6413 - acc: 0.9365 - val_loss: 7.5401 - val_acc: 0.6250\n",
      "Epoch 15/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 6.4711 - acc: 0.9365 - val_loss: 7.3546 - val_acc: 0.6250\n",
      "Epoch 16/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.3069 - acc: 0.9365 - val_loss: 7.2783 - val_acc: 0.6250\n",
      "Epoch 17/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.1460 - acc: 0.9365 - val_loss: 7.1957 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.9900 - acc: 0.9365 - val_loss: 6.9913 - val_acc: 0.6250\n",
      "Epoch 19/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.8354 - acc: 0.9365 - val_loss: 6.7694 - val_acc: 0.6250\n",
      "Epoch 20/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.6861 - acc: 0.9365 - val_loss: 6.6126 - val_acc: 0.6250\n",
      "Epoch 21/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.5402 - acc: 0.9365 - val_loss: 6.5186 - val_acc: 0.6250\n",
      "Epoch 22/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.3981 - acc: 0.9365 - val_loss: 6.3769 - val_acc: 0.6250\n",
      "Epoch 23/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.2592 - acc: 0.9365 - val_loss: 6.2004 - val_acc: 0.6250\n",
      "Epoch 24/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.1233 - acc: 0.9365 - val_loss: 6.0966 - val_acc: 0.6250\n",
      "Epoch 25/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.9905 - acc: 0.9365 - val_loss: 6.0027 - val_acc: 0.6250\n",
      "Epoch 26/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.8616 - acc: 0.9365 - val_loss: 5.8357 - val_acc: 0.6250\n",
      "Epoch 27/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.7360 - acc: 0.9365 - val_loss: 5.7544 - val_acc: 0.6250\n",
      "Epoch 28/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.6138 - acc: 0.9365 - val_loss: 5.5927 - val_acc: 0.6250\n",
      "Epoch 29/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.4951 - acc: 0.9365 - val_loss: 5.5246 - val_acc: 0.6250\n",
      "Epoch 30/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.3804 - acc: 0.9365 - val_loss: 5.3472 - val_acc: 0.6250\n",
      "Epoch 31/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.2690 - acc: 0.9365 - val_loss: 5.3144 - val_acc: 0.6250\n",
      "Epoch 32/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.1609 - acc: 0.9365 - val_loss: 5.1404 - val_acc: 0.6250\n",
      "Epoch 33/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.0550 - acc: 0.9365 - val_loss: 5.0858 - val_acc: 0.6250\n",
      "Epoch 34/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.9522 - acc: 0.9365 - val_loss: 4.9165 - val_acc: 0.6250\n",
      "Epoch 35/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.8526 - acc: 0.9365 - val_loss: 4.8840 - val_acc: 0.6250\n",
      "Epoch 36/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.7560 - acc: 0.9365 - val_loss: 4.7956 - val_acc: 0.6250\n",
      "Epoch 37/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.6621 - acc: 0.9365 - val_loss: 4.5765 - val_acc: 0.6250\n",
      "Epoch 38/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.5718 - acc: 0.9365 - val_loss: 4.4219 - val_acc: 0.6250\n",
      "Epoch 39/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.4831 - acc: 0.9365 - val_loss: 4.5001 - val_acc: 0.6250\n",
      "Epoch 40/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.3997 - acc: 0.9365 - val_loss: 4.2531 - val_acc: 0.6250\n",
      "Epoch 41/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.3147 - acc: 0.9365 - val_loss: 4.1061 - val_acc: 0.6250\n",
      "Epoch 42/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.2341 - acc: 0.9365 - val_loss: 4.1392 - val_acc: 0.6250\n",
      "Epoch 43/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.1553 - acc: 0.9365 - val_loss: 3.9395 - val_acc: 0.6250\n",
      "Epoch 44/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.0814 - acc: 0.9365 - val_loss: 3.8929 - val_acc: 0.6250\n",
      "Epoch 45/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.0064 - acc: 0.9365 - val_loss: 4.0207 - val_acc: 0.6250\n",
      "Epoch 46/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.9387 - acc: 0.9365 - val_loss: 3.7077 - val_acc: 0.6250\n",
      "Epoch 47/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.8685 - acc: 0.9365 - val_loss: 3.7283 - val_acc: 0.6250\n",
      "Epoch 48/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.8027 - acc: 0.9365 - val_loss: 3.9403 - val_acc: 0.6250\n",
      "Epoch 49/1000\n",
      "15619/15619 [==============================] - ETA: 0s - loss: 2.7611 - acc: 0.937 - 0s 11us/step - loss: 2.7464 - acc: 0.9365 - val_loss: 3.4079 - val_acc: 0.6250\n",
      "Epoch 50/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.6879 - acc: 0.9365 - val_loss: 3.7469 - val_acc: 0.6250\n",
      "Epoch 51/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.6188 - acc: 0.9365 - val_loss: 3.3645 - val_acc: 0.6250\n",
      "Epoch 52/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.5583 - acc: 0.9365 - val_loss: 3.5750 - val_acc: 0.6250\n",
      "Epoch 53/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.5002 - acc: 0.9365 - val_loss: 3.2733 - val_acc: 0.6250\n",
      "Epoch 54/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.4448 - acc: 0.9365 - val_loss: 3.4782 - val_acc: 0.6250\n",
      "Epoch 55/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.3929 - acc: 0.9365 - val_loss: 3.1731 - val_acc: 0.6250\n",
      "Epoch 56/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.3397 - acc: 0.9365 - val_loss: 3.3622 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.2894 - acc: 0.9365 - val_loss: 3.1007 - val_acc: 0.6250\n",
      "Epoch 58/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.2397 - acc: 0.9365 - val_loss: 3.2936 - val_acc: 0.6250\n",
      "Epoch 59/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.1927 - acc: 0.9365 - val_loss: 3.0546 - val_acc: 0.6250\n",
      "Epoch 60/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.1438 - acc: 0.9365 - val_loss: 3.1562 - val_acc: 0.6250\n",
      "Epoch 61/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.0992 - acc: 0.9365 - val_loss: 2.9465 - val_acc: 0.6250\n",
      "Epoch 62/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.0550 - acc: 0.9365 - val_loss: 3.1019 - val_acc: 0.6250\n",
      "Epoch 63/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.0140 - acc: 0.9365 - val_loss: 2.7767 - val_acc: 0.6250\n",
      "Epoch 64/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.9770 - acc: 0.9365 - val_loss: 3.0536 - val_acc: 0.6250\n",
      "Epoch 65/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.9351 - acc: 0.9365 - val_loss: 2.7508 - val_acc: 0.6250\n",
      "Epoch 66/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.8945 - acc: 0.9365 - val_loss: 2.8904 - val_acc: 0.6250\n",
      "Epoch 67/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.8573 - acc: 0.9365 - val_loss: 2.8074 - val_acc: 0.6250\n",
      "Epoch 68/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.8180 - acc: 0.9365 - val_loss: 2.6899 - val_acc: 0.6250\n",
      "Epoch 69/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.7820 - acc: 0.9365 - val_loss: 2.7722 - val_acc: 0.6250\n",
      "Epoch 70/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.7472 - acc: 0.9364 - val_loss: 2.5794 - val_acc: 0.6250\n",
      "Epoch 71/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.7152 - acc: 0.9364 - val_loss: 2.6777 - val_acc: 0.6250\n",
      "Epoch 72/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.6808 - acc: 0.9366 - val_loss: 2.6633 - val_acc: 0.6250\n",
      "Epoch 73/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.6485 - acc: 0.9364 - val_loss: 2.4816 - val_acc: 0.6250\n",
      "Epoch 74/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.6229 - acc: 0.9365 - val_loss: 2.4986 - val_acc: 0.6250\n",
      "Epoch 75/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.5877 - acc: 0.9366 - val_loss: 2.6017 - val_acc: 0.6250\n",
      "Epoch 76/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.5601 - acc: 0.9365 - val_loss: 2.6020 - val_acc: 0.6250\n",
      "Epoch 77/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.5320 - acc: 0.9365 - val_loss: 2.4711 - val_acc: 0.6250\n",
      "Epoch 78/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.5014 - acc: 0.9364 - val_loss: 2.3802 - val_acc: 0.6250\n",
      "Epoch 79/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4774 - acc: 0.9364 - val_loss: 2.3503 - val_acc: 0.6250\n",
      "Epoch 80/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4519 - acc: 0.9368 - val_loss: 2.4251 - val_acc: 0.6250\n",
      "Epoch 81/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4231 - acc: 0.9367 - val_loss: 2.5738 - val_acc: 0.6250\n",
      "Epoch 82/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.4119 - acc: 0.9376 - val_loss: 3.1206 - val_acc: 0.6250\n",
      "Epoch 83/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4249 - acc: 0.9365 - val_loss: 2.0597 - val_acc: 0.6250\n",
      "Epoch 84/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.4375 - acc: 0.9415 - val_loss: 3.2330 - val_acc: 0.6250\n",
      "Epoch 85/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4065 - acc: 0.9364 - val_loss: 1.9943 - val_acc: 0.6250\n",
      "Epoch 86/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.3770 - acc: 0.9381 - val_loss: 2.4055 - val_acc: 0.6250\n",
      "Epoch 87/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.3245 - acc: 0.9364 - val_loss: 2.6461 - val_acc: 0.6250\n",
      "Epoch 88/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.3092 - acc: 0.9365 - val_loss: 2.0685 - val_acc: 0.6250\n",
      "Epoch 89/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2906 - acc: 0.9367 - val_loss: 2.0613 - val_acc: 0.6250\n",
      "Epoch 90/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 1.2611 - acc: 0.9365 - val_loss: 2.4849 - val_acc: 0.6250\n",
      "Epoch 91/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2506 - acc: 0.9364 - val_loss: 2.3316 - val_acc: 0.6250\n",
      "Epoch 92/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2202 - acc: 0.9365 - val_loss: 2.0136 - val_acc: 0.6250\n",
      "Epoch 93/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2088 - acc: 0.9373 - val_loss: 2.1916 - val_acc: 0.6250\n",
      "Epoch 94/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1837 - acc: 0.9367 - val_loss: 2.3944 - val_acc: 0.6250\n",
      "Epoch 95/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 1.1708 - acc: 0.9366 - val_loss: 2.0893 - val_acc: 0.6250\n",
      "Epoch 96/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1503 - acc: 0.9369 - val_loss: 2.0288 - val_acc: 0.6250\n",
      "Epoch 97/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.1331 - acc: 0.9367 - val_loss: 2.2341 - val_acc: 0.6250\n",
      "Epoch 98/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1191 - acc: 0.9366 - val_loss: 2.0893 - val_acc: 0.6250\n",
      "Epoch 99/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.1012 - acc: 0.9367 - val_loss: 1.9694 - val_acc: 0.6250\n",
      "Epoch 100/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.0865 - acc: 0.9367 - val_loss: 2.1553 - val_acc: 0.6250\n",
      "Epoch 101/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.0727 - acc: 0.9366 - val_loss: 2.0239 - val_acc: 0.6250\n",
      "Epoch 102/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.0574 - acc: 0.9366 - val_loss: 1.9726 - val_acc: 0.6250\n",
      "Epoch 103/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0434 - acc: 0.9367 - val_loss: 2.0684 - val_acc: 0.6250\n",
      "Epoch 104/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0303 - acc: 0.9366 - val_loss: 1.9267 - val_acc: 0.6250\n",
      "Epoch 105/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.0159 - acc: 0.9367 - val_loss: 2.0147 - val_acc: 0.6250\n",
      "Epoch 106/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.0031 - acc: 0.9367 - val_loss: 1.9235 - val_acc: 0.6250\n",
      "Epoch 107/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9905 - acc: 0.9372 - val_loss: 1.9750 - val_acc: 0.6250\n",
      "Epoch 108/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9779 - acc: 0.9367 - val_loss: 1.9437 - val_acc: 0.6250\n",
      "Epoch 109/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9656 - acc: 0.9374 - val_loss: 1.9076 - val_acc: 0.6250\n",
      "Epoch 110/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9540 - acc: 0.9379 - val_loss: 1.9672 - val_acc: 0.6250\n",
      "Epoch 111/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9428 - acc: 0.9382 - val_loss: 1.9647 - val_acc: 0.6250\n",
      "Epoch 112/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.9314 - acc: 0.9380 - val_loss: 1.9447 - val_acc: 0.6250\n",
      "Epoch 113/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9203 - acc: 0.9394 - val_loss: 2.0133 - val_acc: 0.6250\n",
      "Epoch 114/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9097 - acc: 0.9382 - val_loss: 1.8492 - val_acc: 0.6250\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9013 - acc: 0.9410 - val_loss: 2.0378 - val_acc: 0.6250\n",
      "Epoch 116/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.8917 - acc: 0.9395 - val_loss: 1.9961 - val_acc: 0.6250\n",
      "Epoch 117/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8794 - acc: 0.9389 - val_loss: 1.8492 - val_acc: 0.6250\n",
      "Epoch 118/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8709 - acc: 0.9421 - val_loss: 2.0206 - val_acc: 0.6250\n",
      "Epoch 119/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8594 - acc: 0.9405 - val_loss: 1.9197 - val_acc: 0.6250\n",
      "Epoch 120/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8511 - acc: 0.9412 - val_loss: 1.8087 - val_acc: 0.6250\n",
      "Epoch 121/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8458 - acc: 0.9437 - val_loss: 1.9126 - val_acc: 0.6250\n",
      "Epoch 122/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8315 - acc: 0.9440 - val_loss: 2.0138 - val_acc: 0.6250\n",
      "Epoch 123/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8231 - acc: 0.9416 - val_loss: 1.8426 - val_acc: 0.6250\n",
      "Epoch 124/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8175 - acc: 0.9444 - val_loss: 1.8144 - val_acc: 0.6250\n",
      "Epoch 125/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8085 - acc: 0.9458 - val_loss: 1.9840 - val_acc: 0.6250\n",
      "Epoch 126/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7973 - acc: 0.9445 - val_loss: 2.0831 - val_acc: 0.6250\n",
      "Epoch 127/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7935 - acc: 0.9422 - val_loss: 2.1635 - val_acc: 0.6250\n",
      "Epoch 128/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7858 - acc: 0.9422 - val_loss: 2.0147 - val_acc: 0.6250\n",
      "Epoch 129/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7726 - acc: 0.9442 - val_loss: 1.8143 - val_acc: 0.6250\n",
      "Epoch 130/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7730 - acc: 0.9463 - val_loss: 1.6706 - val_acc: 0.6250\n",
      "Epoch 131/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8115 - acc: 0.9443 - val_loss: 1.8339 - val_acc: 0.6250\n",
      "Epoch 132/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7699 - acc: 0.9495 - val_loss: 2.4285 - val_acc: 0.6250\n",
      "Epoch 133/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7790 - acc: 0.9377 - val_loss: 1.6302 - val_acc: 0.6250\n",
      "Epoch 134/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7619 - acc: 0.9488 - val_loss: 2.0794 - val_acc: 0.6250\n",
      "Epoch 135/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7536 - acc: 0.9375 - val_loss: 1.6970 - val_acc: 0.6250\n",
      "Epoch 136/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7376 - acc: 0.9460 - val_loss: 1.8603 - val_acc: 0.6250\n",
      "Epoch 137/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7296 - acc: 0.9411 - val_loss: 1.7653 - val_acc: 0.6250\n",
      "Epoch 138/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7215 - acc: 0.9462 - val_loss: 1.8405 - val_acc: 0.6250\n",
      "Epoch 139/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7140 - acc: 0.9429 - val_loss: 1.7073 - val_acc: 0.6250\n",
      "Epoch 140/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7071 - acc: 0.9479 - val_loss: 1.9037 - val_acc: 0.6250\n",
      "Epoch 141/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7024 - acc: 0.9440 - val_loss: 1.6350 - val_acc: 0.6250\n",
      "Epoch 142/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6958 - acc: 0.9498 - val_loss: 1.8914 - val_acc: 0.6250\n",
      "Epoch 143/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6890 - acc: 0.9473 - val_loss: 1.7871 - val_acc: 0.6250\n",
      "Epoch 144/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6819 - acc: 0.9482 - val_loss: 1.7004 - val_acc: 0.6250\n",
      "Epoch 145/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6781 - acc: 0.9510 - val_loss: 1.9380 - val_acc: 0.6250\n",
      "Epoch 146/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6684 - acc: 0.9504 - val_loss: 1.9854 - val_acc: 0.6250\n",
      "Epoch 147/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6618 - acc: 0.9503 - val_loss: 1.8705 - val_acc: 0.6250\n",
      "Epoch 148/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6573 - acc: 0.9510 - val_loss: 1.8279 - val_acc: 0.6250\n",
      "Epoch 149/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6557 - acc: 0.9512 - val_loss: 1.7986 - val_acc: 0.6250\n",
      "Epoch 150/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6513 - acc: 0.9526 - val_loss: 1.8566 - val_acc: 0.6250\n",
      "Epoch 151/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6398 - acc: 0.9543 - val_loss: 2.0786 - val_acc: 0.6250\n",
      "Epoch 152/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6468 - acc: 0.9474 - val_loss: 2.6012 - val_acc: 0.6250\n",
      "Epoch 153/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6792 - acc: 0.9420 - val_loss: 2.0301 - val_acc: 0.6250\n",
      "Epoch 154/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6345 - acc: 0.9504 - val_loss: 1.6981 - val_acc: 0.6250\n",
      "Epoch 155/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6291 - acc: 0.9522 - val_loss: 1.9790 - val_acc: 0.6250\n",
      "Epoch 156/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6257 - acc: 0.9491 - val_loss: 1.7015 - val_acc: 0.6250\n",
      "Epoch 157/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6170 - acc: 0.9523 - val_loss: 1.9581 - val_acc: 0.6250\n",
      "Epoch 158/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6107 - acc: 0.9534 - val_loss: 2.0429 - val_acc: 0.6250\n",
      "Epoch 159/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6064 - acc: 0.9523 - val_loss: 1.8139 - val_acc: 0.6250\n",
      "Epoch 160/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5984 - acc: 0.9545 - val_loss: 1.8868 - val_acc: 0.6250\n",
      "Epoch 161/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5986 - acc: 0.9527 - val_loss: 2.1885 - val_acc: 0.6250\n",
      "Epoch 162/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6115 - acc: 0.9475 - val_loss: 2.1353 - val_acc: 0.6250\n",
      "Epoch 163/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5917 - acc: 0.9525 - val_loss: 1.7121 - val_acc: 0.6250\n",
      "Epoch 164/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5948 - acc: 0.9508 - val_loss: 1.8025 - val_acc: 0.6250\n",
      "Epoch 165/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5804 - acc: 0.9549 - val_loss: 2.1563 - val_acc: 0.6250\n",
      "Epoch 166/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5840 - acc: 0.9513 - val_loss: 1.8999 - val_acc: 0.6250\n",
      "Epoch 167/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5712 - acc: 0.9552 - val_loss: 1.8192 - val_acc: 0.6250\n",
      "Epoch 168/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5663 - acc: 0.9552 - val_loss: 1.8728 - val_acc: 0.6250\n",
      "Epoch 169/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5623 - acc: 0.9554 - val_loss: 2.1060 - val_acc: 0.6250\n",
      "Epoch 170/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5813 - acc: 0.9483 - val_loss: 3.2489 - val_acc: 0.6250\n",
      "Epoch 171/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6722 - acc: 0.9417 - val_loss: 1.5091 - val_acc: 0.5000\n",
      "Epoch 172/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6719 - acc: 0.9268 - val_loss: 2.9758 - val_acc: 0.6250\n",
      "Epoch 173/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6601 - acc: 0.9390 - val_loss: 1.3117 - val_acc: 0.6250\n",
      "Epoch 174/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6447 - acc: 0.9439 - val_loss: 1.7723 - val_acc: 0.6250\n",
      "Epoch 175/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5978 - acc: 0.9403 - val_loss: 2.2300 - val_acc: 0.6250\n",
      "Epoch 176/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6062 - acc: 0.9387 - val_loss: 1.4191 - val_acc: 0.6250\n",
      "Epoch 177/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5924 - acc: 0.9463 - val_loss: 1.3815 - val_acc: 0.6250\n",
      "Epoch 178/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5817 - acc: 0.9450 - val_loss: 1.8943 - val_acc: 0.6250\n",
      "Epoch 179/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5826 - acc: 0.9394 - val_loss: 1.8147 - val_acc: 0.6250\n",
      "Epoch 180/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5677 - acc: 0.9407 - val_loss: 1.4220 - val_acc: 0.6250\n",
      "Epoch 181/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5690 - acc: 0.9471 - val_loss: 1.5146 - val_acc: 0.6250\n",
      "Epoch 182/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5554 - acc: 0.9461 - val_loss: 1.8446 - val_acc: 0.6250\n",
      "Epoch 183/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5560 - acc: 0.9428 - val_loss: 1.6161 - val_acc: 0.6250\n",
      "Epoch 184/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5461 - acc: 0.9473 - val_loss: 1.4047 - val_acc: 0.6250\n",
      "Epoch 185/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5423 - acc: 0.9493 - val_loss: 1.6218 - val_acc: 0.6250\n",
      "Epoch 186/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5382 - acc: 0.9443 - val_loss: 1.5359 - val_acc: 0.6250\n",
      "Epoch 187/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5295 - acc: 0.9488 - val_loss: 1.3715 - val_acc: 0.6250\n",
      "Epoch 188/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5282 - acc: 0.9513 - val_loss: 1.6417 - val_acc: 0.6250\n",
      "Epoch 189/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5241 - acc: 0.9446 - val_loss: 1.5662 - val_acc: 0.6250\n",
      "Epoch 190/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5172 - acc: 0.9497 - val_loss: 1.5231 - val_acc: 0.6250\n",
      "Epoch 191/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5127 - acc: 0.9518 - val_loss: 1.7852 - val_acc: 0.6250\n",
      "Epoch 192/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5104 - acc: 0.9494 - val_loss: 1.5605 - val_acc: 0.6250\n",
      "Epoch 193/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5067 - acc: 0.9539 - val_loss: 1.7601 - val_acc: 0.6250\n",
      "Epoch 194/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5018 - acc: 0.9512 - val_loss: 1.7072 - val_acc: 0.6250\n",
      "Epoch 195/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4977 - acc: 0.9539 - val_loss: 1.7244 - val_acc: 0.6250\n",
      "Epoch 196/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4933 - acc: 0.9532 - val_loss: 1.8198 - val_acc: 0.6250\n",
      "Epoch 197/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4910 - acc: 0.9547 - val_loss: 1.7716 - val_acc: 0.6250\n",
      "Epoch 198/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4865 - acc: 0.9547 - val_loss: 1.8493 - val_acc: 0.6250\n",
      "Epoch 199/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4835 - acc: 0.9551 - val_loss: 1.8164 - val_acc: 0.6250\n",
      "Epoch 200/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4803 - acc: 0.9555 - val_loss: 1.8193 - val_acc: 0.6250\n",
      "Epoch 201/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4771 - acc: 0.9559 - val_loss: 1.8891 - val_acc: 0.6250\n",
      "Epoch 202/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4742 - acc: 0.9553 - val_loss: 1.7320 - val_acc: 0.6250\n",
      "Epoch 203/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4747 - acc: 0.9550 - val_loss: 1.8268 - val_acc: 0.6250\n",
      "Epoch 204/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4678 - acc: 0.9561 - val_loss: 1.8545 - val_acc: 0.6250\n",
      "Epoch 205/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4644 - acc: 0.9567 - val_loss: 1.8417 - val_acc: 0.6250\n",
      "Epoch 206/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4615 - acc: 0.9568 - val_loss: 1.7981 - val_acc: 0.6250\n",
      "Epoch 207/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4591 - acc: 0.9567 - val_loss: 1.8077 - val_acc: 0.6250\n",
      "Epoch 208/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4563 - acc: 0.9570 - val_loss: 1.8402 - val_acc: 0.6250\n",
      "Epoch 209/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4539 - acc: 0.9565 - val_loss: 1.8004 - val_acc: 0.6250\n",
      "Epoch 210/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4578 - acc: 0.9549 - val_loss: 1.6448 - val_acc: 0.6250\n",
      "Epoch 211/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4927 - acc: 0.9472 - val_loss: 1.8601 - val_acc: 0.6250\n",
      "Epoch 212/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4634 - acc: 0.9527 - val_loss: 2.3038 - val_acc: 0.6250\n",
      "Epoch 213/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.4615 - acc: 0.9533 - val_loss: 1.5369 - val_acc: 0.6250\n",
      "Epoch 214/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4567 - acc: 0.9547 - val_loss: 1.9377 - val_acc: 0.6250\n",
      "Epoch 215/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4536 - acc: 0.9518 - val_loss: 1.5916 - val_acc: 0.6250\n",
      "Epoch 216/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4500 - acc: 0.9553 - val_loss: 1.8915 - val_acc: 0.6250\n",
      "Epoch 217/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4425 - acc: 0.9558 - val_loss: 1.7950 - val_acc: 0.6250\n",
      "Epoch 218/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4388 - acc: 0.9561 - val_loss: 1.6183 - val_acc: 0.6250\n",
      "Epoch 219/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4367 - acc: 0.9557 - val_loss: 1.9080 - val_acc: 0.6250\n",
      "Epoch 220/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4339 - acc: 0.9561 - val_loss: 1.8568 - val_acc: 0.6250\n",
      "Epoch 221/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4298 - acc: 0.9563 - val_loss: 1.8309 - val_acc: 0.6250\n",
      "Epoch 222/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4309 - acc: 0.9556 - val_loss: 1.8547 - val_acc: 0.6250\n",
      "Epoch 223/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4254 - acc: 0.9571 - val_loss: 1.9733 - val_acc: 0.6250\n",
      "Epoch 224/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4250 - acc: 0.9561 - val_loss: 2.0387 - val_acc: 0.6250\n",
      "Epoch 225/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4240 - acc: 0.9559 - val_loss: 1.8713 - val_acc: 0.6250\n",
      "Epoch 226/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4188 - acc: 0.9565 - val_loss: 1.7695 - val_acc: 0.6250\n",
      "Epoch 227/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4188 - acc: 0.9564 - val_loss: 1.7745 - val_acc: 0.6250\n",
      "Epoch 228/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4140 - acc: 0.9563 - val_loss: 1.9900 - val_acc: 0.6250\n",
      "Epoch 229/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4185 - acc: 0.9558 - val_loss: 2.4429 - val_acc: 0.6250\n",
      "Epoch 230/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4358 - acc: 0.9511 - val_loss: 2.0687 - val_acc: 0.6250\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.4158 - acc: 0.9560 - val_loss: 1.6440 - val_acc: 0.6250\n",
      "Epoch 232/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4332 - acc: 0.9519 - val_loss: 1.7668 - val_acc: 0.6250\n",
      "Epoch 233/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4103 - acc: 0.9564 - val_loss: 2.0435 - val_acc: 0.6250\n",
      "Epoch 234/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4142 - acc: 0.9547 - val_loss: 1.6216 - val_acc: 0.6250\n",
      "Epoch 235/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4142 - acc: 0.9556 - val_loss: 1.9352 - val_acc: 0.6250\n",
      "Epoch 236/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4098 - acc: 0.9551 - val_loss: 2.0923 - val_acc: 0.6250\n",
      "Epoch 237/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4090 - acc: 0.9557 - val_loss: 1.6142 - val_acc: 0.6250\n",
      "Epoch 238/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4145 - acc: 0.9544 - val_loss: 1.9193 - val_acc: 0.6250\n",
      "Epoch 239/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4064 - acc: 0.9554 - val_loss: 2.0762 - val_acc: 0.6250\n",
      "Epoch 240/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4060 - acc: 0.9556 - val_loss: 1.5634 - val_acc: 0.6250\n",
      "Epoch 241/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.4095 - acc: 0.9546 - val_loss: 1.9695 - val_acc: 0.6250\n",
      "Epoch 242/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4033 - acc: 0.9552 - val_loss: 1.8003 - val_acc: 0.6250\n",
      "Epoch 243/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3930 - acc: 0.9568 - val_loss: 1.6799 - val_acc: 0.6250\n",
      "Epoch 244/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3911 - acc: 0.9565 - val_loss: 1.9341 - val_acc: 0.6250\n",
      "Epoch 245/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3904 - acc: 0.9565 - val_loss: 1.6859 - val_acc: 0.6250\n",
      "Epoch 246/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3892 - acc: 0.9558 - val_loss: 1.8309 - val_acc: 0.6250\n",
      "Epoch 247/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.3836 - acc: 0.9574 - val_loss: 1.7826 - val_acc: 0.6250\n",
      "Epoch 248/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3831 - acc: 0.9563 - val_loss: 1.6959 - val_acc: 0.6250\n",
      "Epoch 249/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3840 - acc: 0.9565 - val_loss: 1.8947 - val_acc: 0.6250\n",
      "Epoch 250/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.3825 - acc: 0.9561 - val_loss: 2.1228 - val_acc: 0.6250\n",
      "Epoch 251/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3857 - acc: 0.9561 - val_loss: 1.6961 - val_acc: 0.6250\n",
      "Epoch 252/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3865 - acc: 0.9556 - val_loss: 1.7082 - val_acc: 0.6250\n",
      "Epoch 253/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.3780 - acc: 0.9565 - val_loss: 2.0233 - val_acc: 0.6250\n",
      "Epoch 254/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3765 - acc: 0.9574 - val_loss: 1.7112 - val_acc: 0.6250\n",
      "Epoch 255/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3809 - acc: 0.9556 - val_loss: 1.8470 - val_acc: 0.6250\n",
      "Epoch 256/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3725 - acc: 0.9566 - val_loss: 2.1549 - val_acc: 0.6250\n",
      "Epoch 257/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3774 - acc: 0.9563 - val_loss: 1.7422 - val_acc: 0.6250\n",
      "Epoch 258/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3723 - acc: 0.9561 - val_loss: 1.7082 - val_acc: 0.6250\n",
      "Epoch 259/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3678 - acc: 0.9571 - val_loss: 2.0305 - val_acc: 0.6250\n",
      "Epoch 260/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3682 - acc: 0.9567 - val_loss: 2.0224 - val_acc: 0.6250\n",
      "Epoch 261/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3638 - acc: 0.9567 - val_loss: 1.7827 - val_acc: 0.6250\n",
      "Epoch 262/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3716 - acc: 0.9556 - val_loss: 1.7149 - val_acc: 0.6250\n",
      "Epoch 263/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3685 - acc: 0.9558 - val_loss: 1.8447 - val_acc: 0.6250\n",
      "Epoch 264/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3596 - acc: 0.9570 - val_loss: 1.9675 - val_acc: 0.6250\n",
      "Epoch 265/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3590 - acc: 0.9568 - val_loss: 1.7273 - val_acc: 0.6250\n",
      "Epoch 266/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3642 - acc: 0.9558 - val_loss: 1.7783 - val_acc: 0.6250\n",
      "Epoch 267/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3568 - acc: 0.9569 - val_loss: 2.1422 - val_acc: 0.6250\n",
      "Epoch 268/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3662 - acc: 0.9560 - val_loss: 1.8696 - val_acc: 0.6250\n",
      "Epoch 269/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3547 - acc: 0.9563 - val_loss: 1.7116 - val_acc: 0.6250\n",
      "Epoch 270/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3612 - acc: 0.9558 - val_loss: 1.8358 - val_acc: 0.6250\n",
      "Epoch 271/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3515 - acc: 0.9567 - val_loss: 2.0928 - val_acc: 0.6250\n",
      "Epoch 272/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3543 - acc: 0.9560 - val_loss: 1.8793 - val_acc: 0.6250\n",
      "Epoch 273/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3491 - acc: 0.9567 - val_loss: 1.7042 - val_acc: 0.6250\n",
      "Epoch 274/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3697 - acc: 0.9541 - val_loss: 1.6900 - val_acc: 0.6250\n",
      "Epoch 275/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3549 - acc: 0.9558 - val_loss: 1.9762 - val_acc: 0.6250\n",
      "Epoch 276/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3529 - acc: 0.9560 - val_loss: 1.7914 - val_acc: 0.6250\n",
      "Epoch 277/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3451 - acc: 0.9568 - val_loss: 1.8418 - val_acc: 0.6250\n",
      "Epoch 278/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3436 - acc: 0.9572 - val_loss: 2.0520 - val_acc: 0.6250\n",
      "Epoch 279/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3432 - acc: 0.9570 - val_loss: 1.8946 - val_acc: 0.6250\n",
      "Epoch 280/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3414 - acc: 0.9564 - val_loss: 1.8614 - val_acc: 0.6250\n",
      "Epoch 281/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3394 - acc: 0.9567 - val_loss: 2.1006 - val_acc: 0.6250\n",
      "Epoch 282/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3466 - acc: 0.9561 - val_loss: 2.1950 - val_acc: 0.6250\n",
      "Epoch 283/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3452 - acc: 0.9560 - val_loss: 2.0735 - val_acc: 0.6250\n",
      "Epoch 284/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3378 - acc: 0.9562 - val_loss: 1.9412 - val_acc: 0.6250\n",
      "Epoch 285/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3349 - acc: 0.9568 - val_loss: 1.7992 - val_acc: 0.6250\n",
      "Epoch 286/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3367 - acc: 0.9562 - val_loss: 1.8368 - val_acc: 0.6250\n",
      "Epoch 287/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3328 - acc: 0.9566 - val_loss: 1.9755 - val_acc: 0.6250\n",
      "Epoch 288/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3334 - acc: 0.9567 - val_loss: 2.0701 - val_acc: 0.6250\n",
      "Epoch 289/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3362 - acc: 0.9559 - val_loss: 2.0473 - val_acc: 0.6250\n",
      "Epoch 290/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3304 - acc: 0.9568 - val_loss: 1.8480 - val_acc: 0.6250\n",
      "Epoch 291/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3465 - acc: 0.9540 - val_loss: 1.7090 - val_acc: 0.6250\n",
      "Epoch 292/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3713 - acc: 0.9504 - val_loss: 2.1168 - val_acc: 0.6250\n",
      "Epoch 293/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3658 - acc: 0.9510 - val_loss: 2.7166 - val_acc: 0.6250\n",
      "Epoch 294/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.4062 - acc: 0.9428 - val_loss: 1.5066 - val_acc: 0.6250\n",
      "Epoch 295/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3624 - acc: 0.9524 - val_loss: 2.1398 - val_acc: 0.6250\n",
      "Epoch 296/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3671 - acc: 0.9472 - val_loss: 1.7955 - val_acc: 0.6250\n",
      "Epoch 297/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3492 - acc: 0.9540 - val_loss: 1.4257 - val_acc: 0.6250\n",
      "Epoch 298/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3456 - acc: 0.9539 - val_loss: 1.9702 - val_acc: 0.6250\n",
      "Epoch 299/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3473 - acc: 0.9515 - val_loss: 1.3396 - val_acc: 0.6250\n",
      "Epoch 300/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3449 - acc: 0.9542 - val_loss: 1.7115 - val_acc: 0.6250\n",
      "Epoch 301/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3407 - acc: 0.9535 - val_loss: 1.5523 - val_acc: 0.6250\n",
      "Epoch 302/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3334 - acc: 0.9566 - val_loss: 1.4507 - val_acc: 0.6250\n",
      "Epoch 303/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3314 - acc: 0.9560 - val_loss: 1.7513 - val_acc: 0.6250\n",
      "Epoch 304/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3334 - acc: 0.9559 - val_loss: 1.4835 - val_acc: 0.6250\n",
      "Epoch 305/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3294 - acc: 0.9565 - val_loss: 1.7882 - val_acc: 0.6250\n",
      "Epoch 306/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.3273 - acc: 0.9563 - val_loss: 1.5718 - val_acc: 0.6250\n",
      "Epoch 307/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3235 - acc: 0.9566 - val_loss: 1.7524 - val_acc: 0.6250\n",
      "Epoch 308/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3212 - acc: 0.9569 - val_loss: 1.6693 - val_acc: 0.6250\n",
      "Epoch 309/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3190 - acc: 0.9569 - val_loss: 1.8037 - val_acc: 0.6250\n",
      "Epoch 310/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3182 - acc: 0.9568 - val_loss: 1.7055 - val_acc: 0.6250\n",
      "Epoch 311/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3167 - acc: 0.9565 - val_loss: 1.9312 - val_acc: 0.6250\n",
      "Epoch 312/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3162 - acc: 0.9568 - val_loss: 1.7272 - val_acc: 0.6250\n",
      "Epoch 313/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3167 - acc: 0.9568 - val_loss: 1.9733 - val_acc: 0.6250\n",
      "Epoch 314/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3139 - acc: 0.9568 - val_loss: 1.7985 - val_acc: 0.6250\n",
      "Epoch 315/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3124 - acc: 0.9572 - val_loss: 2.0028 - val_acc: 0.6250\n",
      "Epoch 316/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3127 - acc: 0.9562 - val_loss: 1.9267 - val_acc: 0.6250\n",
      "Epoch 317/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3094 - acc: 0.9566 - val_loss: 1.8060 - val_acc: 0.6250\n",
      "Epoch 318/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3082 - acc: 0.9567 - val_loss: 2.0221 - val_acc: 0.6250\n",
      "Epoch 319/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3084 - acc: 0.9568 - val_loss: 1.7877 - val_acc: 0.6250\n",
      "Epoch 320/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3120 - acc: 0.9560 - val_loss: 1.9370 - val_acc: 0.6250\n",
      "Epoch 321/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3059 - acc: 0.9572 - val_loss: 2.0521 - val_acc: 0.6250\n",
      "Epoch 322/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3062 - acc: 0.9567 - val_loss: 1.7570 - val_acc: 0.6250\n",
      "Epoch 323/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3073 - acc: 0.9565 - val_loss: 2.0064 - val_acc: 0.6250\n",
      "Epoch 324/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3049 - acc: 0.9568 - val_loss: 1.9580 - val_acc: 0.6250\n",
      "Epoch 325/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3024 - acc: 0.9567 - val_loss: 1.7724 - val_acc: 0.6250\n",
      "Epoch 326/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3029 - acc: 0.9568 - val_loss: 2.0004 - val_acc: 0.6250\n",
      "Epoch 327/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3018 - acc: 0.9567 - val_loss: 1.8777 - val_acc: 0.6250\n",
      "Epoch 328/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2982 - acc: 0.9569 - val_loss: 1.8962 - val_acc: 0.6250\n",
      "Epoch 329/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2971 - acc: 0.9571 - val_loss: 1.8813 - val_acc: 0.6250\n",
      "Epoch 330/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2978 - acc: 0.9567 - val_loss: 1.8453 - val_acc: 0.6250\n",
      "Epoch 331/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2997 - acc: 0.9558 - val_loss: 2.0236 - val_acc: 0.6250\n",
      "Epoch 332/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2957 - acc: 0.9564 - val_loss: 2.1787 - val_acc: 0.6250\n",
      "Epoch 333/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2966 - acc: 0.9567 - val_loss: 1.8489 - val_acc: 0.6250\n",
      "Epoch 334/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3034 - acc: 0.9568 - val_loss: 1.7258 - val_acc: 0.6250\n",
      "Epoch 335/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2984 - acc: 0.9560 - val_loss: 1.9439 - val_acc: 0.6250\n",
      "Epoch 336/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2966 - acc: 0.9567 - val_loss: 2.0676 - val_acc: 0.6250\n",
      "Epoch 337/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2958 - acc: 0.9564 - val_loss: 1.7869 - val_acc: 0.6250\n",
      "Epoch 338/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2974 - acc: 0.9558 - val_loss: 1.9352 - val_acc: 0.6250\n",
      "Epoch 339/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2926 - acc: 0.9570 - val_loss: 2.0522 - val_acc: 0.6250\n",
      "Epoch 340/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2934 - acc: 0.9565 - val_loss: 1.7196 - val_acc: 0.6250\n",
      "Epoch 341/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3017 - acc: 0.9558 - val_loss: 1.9011 - val_acc: 0.6250\n",
      "Epoch 342/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2957 - acc: 0.9558 - val_loss: 2.2678 - val_acc: 0.6250\n",
      "Epoch 343/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3071 - acc: 0.9554 - val_loss: 1.5805 - val_acc: 0.6250\n",
      "Epoch 344/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3120 - acc: 0.9551 - val_loss: 2.0974 - val_acc: 0.6250\n",
      "Epoch 345/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.3033 - acc: 0.9542 - val_loss: 1.8539 - val_acc: 0.6250\n",
      "Epoch 346/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2920 - acc: 0.9567 - val_loss: 1.7789 - val_acc: 0.6250\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2946 - acc: 0.9567 - val_loss: 2.0726 - val_acc: 0.6250\n",
      "Epoch 348/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2976 - acc: 0.9561 - val_loss: 1.6466 - val_acc: 0.6250\n",
      "Epoch 349/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2961 - acc: 0.9562 - val_loss: 1.9999 - val_acc: 0.6250\n",
      "Epoch 350/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2903 - acc: 0.9563 - val_loss: 1.5752 - val_acc: 0.6250\n",
      "Epoch 351/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2938 - acc: 0.9558 - val_loss: 1.9317 - val_acc: 0.6250\n",
      "Epoch 352/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2891 - acc: 0.9561 - val_loss: 1.8160 - val_acc: 0.6250\n",
      "Epoch 353/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2851 - acc: 0.9569 - val_loss: 1.9249 - val_acc: 0.6250\n",
      "Epoch 354/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2855 - acc: 0.9566 - val_loss: 2.0306 - val_acc: 0.6250\n",
      "Epoch 355/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2855 - acc: 0.9564 - val_loss: 1.6774 - val_acc: 0.6250\n",
      "Epoch 356/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2843 - acc: 0.9572 - val_loss: 1.8778 - val_acc: 0.6250\n",
      "Epoch 357/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2814 - acc: 0.9567 - val_loss: 1.7420 - val_acc: 0.6250\n",
      "Epoch 358/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2824 - acc: 0.9560 - val_loss: 1.9763 - val_acc: 0.6250\n",
      "Epoch 359/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2812 - acc: 0.9560 - val_loss: 1.9126 - val_acc: 0.6250\n",
      "Epoch 360/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2785 - acc: 0.9568 - val_loss: 1.7895 - val_acc: 0.6250\n",
      "Epoch 361/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2773 - acc: 0.9572 - val_loss: 1.8869 - val_acc: 0.6250\n",
      "Epoch 362/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2767 - acc: 0.9565 - val_loss: 1.8101 - val_acc: 0.6250\n",
      "Epoch 363/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2768 - acc: 0.9561 - val_loss: 1.9215 - val_acc: 0.6250\n",
      "Epoch 364/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2758 - acc: 0.9565 - val_loss: 2.0922 - val_acc: 0.6250\n",
      "Epoch 365/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2798 - acc: 0.9565 - val_loss: 1.8522 - val_acc: 0.6250\n",
      "Epoch 366/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2738 - acc: 0.9565 - val_loss: 1.9251 - val_acc: 0.6250\n",
      "Epoch 367/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2740 - acc: 0.9572 - val_loss: 2.0258 - val_acc: 0.6250\n",
      "Epoch 368/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2742 - acc: 0.9568 - val_loss: 1.7704 - val_acc: 0.6250\n",
      "Epoch 369/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2840 - acc: 0.9562 - val_loss: 1.8921 - val_acc: 0.6250\n",
      "Epoch 370/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2757 - acc: 0.9563 - val_loss: 2.3353 - val_acc: 0.6250\n",
      "Epoch 371/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2868 - acc: 0.9558 - val_loss: 1.6210 - val_acc: 0.6250\n",
      "Epoch 372/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.3038 - acc: 0.9527 - val_loss: 2.0107 - val_acc: 0.6250\n",
      "Epoch 373/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2920 - acc: 0.9538 - val_loss: 1.9557 - val_acc: 0.6250\n",
      "Epoch 374/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2832 - acc: 0.9558 - val_loss: 1.6688 - val_acc: 0.6250\n",
      "Epoch 375/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2794 - acc: 0.9567 - val_loss: 2.0925 - val_acc: 0.6250\n",
      "Epoch 376/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2833 - acc: 0.9560 - val_loss: 1.6301 - val_acc: 0.6250\n",
      "Epoch 377/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2774 - acc: 0.9560 - val_loss: 1.9829 - val_acc: 0.6250\n",
      "Epoch 378/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2800 - acc: 0.9560 - val_loss: 1.5830 - val_acc: 0.6250\n",
      "Epoch 379/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2750 - acc: 0.9562 - val_loss: 1.9061 - val_acc: 0.6250\n",
      "Epoch 380/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2738 - acc: 0.9561 - val_loss: 1.6423 - val_acc: 0.6250\n",
      "Epoch 381/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2704 - acc: 0.9569 - val_loss: 1.9443 - val_acc: 0.6250\n",
      "Epoch 382/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2696 - acc: 0.9570 - val_loss: 1.7102 - val_acc: 0.6250\n",
      "Epoch 383/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2709 - acc: 0.9559 - val_loss: 1.8572 - val_acc: 0.6250\n",
      "Epoch 384/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2671 - acc: 0.9567 - val_loss: 1.8198 - val_acc: 0.6250\n",
      "Epoch 385/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2655 - acc: 0.9563 - val_loss: 1.8193 - val_acc: 0.6250\n",
      "Epoch 386/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2646 - acc: 0.9569 - val_loss: 1.9186 - val_acc: 0.6250\n",
      "Epoch 387/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2643 - acc: 0.9568 - val_loss: 1.8063 - val_acc: 0.6250\n",
      "Epoch 388/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2628 - acc: 0.9570 - val_loss: 1.8956 - val_acc: 0.6250\n",
      "Epoch 389/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2624 - acc: 0.9571 - val_loss: 1.7678 - val_acc: 0.6250\n",
      "Epoch 390/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2689 - acc: 0.9558 - val_loss: 1.7346 - val_acc: 0.6250\n",
      "Epoch 391/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2645 - acc: 0.9562 - val_loss: 1.9694 - val_acc: 0.6250\n",
      "Epoch 392/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2626 - acc: 0.9569 - val_loss: 1.8297 - val_acc: 0.6250\n",
      "Epoch 393/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2618 - acc: 0.9567 - val_loss: 1.8518 - val_acc: 0.6250\n",
      "Epoch 394/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2605 - acc: 0.9568 - val_loss: 2.0044 - val_acc: 0.6250\n",
      "Epoch 395/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2623 - acc: 0.9567 - val_loss: 1.7926 - val_acc: 0.6250\n",
      "Epoch 396/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2615 - acc: 0.9565 - val_loss: 1.8640 - val_acc: 0.6250\n",
      "Epoch 397/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2625 - acc: 0.9563 - val_loss: 2.1953 - val_acc: 0.6250\n",
      "Epoch 398/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2701 - acc: 0.9561 - val_loss: 1.6357 - val_acc: 0.6250\n",
      "Epoch 399/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2795 - acc: 0.9544 - val_loss: 1.9432 - val_acc: 0.6250\n",
      "Epoch 400/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2743 - acc: 0.9533 - val_loss: 2.1683 - val_acc: 0.6250\n",
      "Epoch 401/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2739 - acc: 0.9556 - val_loss: 1.6015 - val_acc: 0.6250\n",
      "Epoch 402/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2652 - acc: 0.9583 - val_loss: 2.0406 - val_acc: 0.6250\n",
      "Epoch 403/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2651 - acc: 0.9563 - val_loss: 1.6017 - val_acc: 0.6250\n",
      "Epoch 404/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2617 - acc: 0.9576 - val_loss: 1.8838 - val_acc: 0.6250\n",
      "Epoch 405/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2592 - acc: 0.9566 - val_loss: 1.6278 - val_acc: 0.6250\n",
      "Epoch 406/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.2582 - acc: 0.9575 - val_loss: 1.8471 - val_acc: 0.6250\n",
      "Epoch 407/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2563 - acc: 0.9568 - val_loss: 1.7161 - val_acc: 0.6250\n",
      "Epoch 408/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2556 - acc: 0.9571 - val_loss: 1.8877 - val_acc: 0.6250\n",
      "Epoch 409/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2570 - acc: 0.9566 - val_loss: 2.0254 - val_acc: 0.6250\n",
      "Epoch 410/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2573 - acc: 0.9568 - val_loss: 1.7106 - val_acc: 0.6250\n",
      "Epoch 411/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2572 - acc: 0.9565 - val_loss: 1.8818 - val_acc: 0.6250\n",
      "Epoch 412/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2548 - acc: 0.9572 - val_loss: 1.8146 - val_acc: 0.6250\n",
      "Epoch 413/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2534 - acc: 0.9567 - val_loss: 1.7795 - val_acc: 0.6250\n",
      "Epoch 414/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2520 - acc: 0.9579 - val_loss: 1.8558 - val_acc: 0.6250\n",
      "Epoch 415/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2509 - acc: 0.9569 - val_loss: 1.8971 - val_acc: 0.6250\n",
      "Epoch 416/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2504 - acc: 0.9569 - val_loss: 1.9537 - val_acc: 0.6250\n",
      "Epoch 417/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2504 - acc: 0.9561 - val_loss: 1.8370 - val_acc: 0.6250\n",
      "Epoch 418/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2504 - acc: 0.9567 - val_loss: 1.8290 - val_acc: 0.6250\n",
      "Epoch 419/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2491 - acc: 0.9568 - val_loss: 1.8983 - val_acc: 0.6250\n",
      "Epoch 420/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2482 - acc: 0.9565 - val_loss: 1.8914 - val_acc: 0.6250\n",
      "Epoch 421/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.2476 - acc: 0.9568 - val_loss: 2.0606 - val_acc: 0.6250\n",
      "Epoch 422/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2542 - acc: 0.9565 - val_loss: 1.9737 - val_acc: 0.6250\n",
      "Epoch 423/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2482 - acc: 0.9564 - val_loss: 1.8510 - val_acc: 0.6250\n",
      "Epoch 424/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2515 - acc: 0.9565 - val_loss: 1.9450 - val_acc: 0.6250\n",
      "Epoch 425/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2502 - acc: 0.9570 - val_loss: 2.3685 - val_acc: 0.6250\n",
      "Epoch 426/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2602 - acc: 0.9568 - val_loss: 1.6670 - val_acc: 0.6250\n",
      "Epoch 427/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2734 - acc: 0.9560 - val_loss: 2.0007 - val_acc: 0.6250\n",
      "Epoch 428/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2664 - acc: 0.9534 - val_loss: 1.9469 - val_acc: 0.6250\n",
      "Epoch 429/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2589 - acc: 0.9558 - val_loss: 1.6256 - val_acc: 0.6250\n",
      "Epoch 430/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2551 - acc: 0.9581 - val_loss: 2.2427 - val_acc: 0.6250\n",
      "Epoch 431/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2652 - acc: 0.9554 - val_loss: 1.5258 - val_acc: 0.6250\n",
      "Epoch 432/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2590 - acc: 0.9565 - val_loss: 1.9229 - val_acc: 0.6250\n",
      "Epoch 433/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2535 - acc: 0.9564 - val_loss: 1.5222 - val_acc: 0.6250\n",
      "Epoch 434/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2501 - acc: 0.9568 - val_loss: 1.8244 - val_acc: 0.6250\n",
      "Epoch 435/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2481 - acc: 0.9570 - val_loss: 1.5785 - val_acc: 0.6250\n",
      "Epoch 436/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.2523 - acc: 0.9584 - val_loss: 1.9575 - val_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    #tf.compat.v1.keras.backend.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model_bert.fit(all_embeddings, y, epochs=1000, batch_size=10000, \n",
    "                             validation_split = 0.001, callbacks=cb_list)\n",
    "    model_bert.save_weights('../model/bert_ner_logistic_twitter/model_bert_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#acc = history.history['acc']\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000\n",
    "batches = math.ceil(all_embeddings_test.shape[0] / bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Batch 1\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_probs = []\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    model_bert.load_weights('../model/bert_ner_logistic_twitter/model_bert_weights.h5')\n",
    "\n",
    "    for i in range(1,batches+1):\n",
    "        print(\"Predicting Batch\",i)\n",
    "        new_text_pr = all_embeddings_test[(i-1)*bs:i*bs]\n",
    "        preds = model_bert.predict(new_text_pr)\n",
    "        all_probs.append(preds)\n",
    "        preds = encoder.inverse_transform(np.argmax(preds,axis=1))\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate(all_preds, axis=0)\n",
    "results_probs = np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/NER/bert_logistic_twitter/test_results.tsv\", results_probs, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/NER/bert_logistic_twitter/test_predictions.tsv\", results, delimiter=\"\\t\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9498234528897974\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",sum(results==y_test)/results.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230219018127863"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, results, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_results = np.full((5381,), 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5381,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.242066915923689"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, most_common_results, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
