{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Hugging Face Transformers\n",
    "\n",
    "https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import transformers as ppb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-cased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/SENT/train.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(\"../datasets/SENT/test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long sentences to 128 tokens\n",
    "X = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y = np.array(df[1])\n",
    "del df\n",
    "\n",
    "X_test = test_df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y_test = np.array(test_df[1])\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding of y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "y = encoder.transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# One hot Encoding of y test\n",
    "y_oh = encoder.transform(y_test)\n",
    "y_oh = to_categorical(y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(tokenizedBatch):\n",
    "    max_len = 0\n",
    "    for i in tokenizedBatch.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenizedBatch.values])\n",
    "    \n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    input_ids = torch.tensor(padded).to(torch.long)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 1000\n",
    "# all_embeddings = []\n",
    "# all_embeddings_test = []\n",
    "\n",
    "# # Process Training Set Embeddings\n",
    "# batches = math.ceil(X.shape[0] / BATCH_SIZE)\n",
    "\n",
    "# for i in range(1, batches+1):\n",
    "#     print(\"Generating Embeddings for Batch:\",i,\"of\", batches)\n",
    "#     batchEmbeddings = GetEmbeddings(X[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "#     all_embeddings.append(batchEmbeddings)\n",
    "\n",
    "# # Process Test Set Embeddings\n",
    "# batches = math.ceil(X_test.shape[0] / BATCH_SIZE)\n",
    "\n",
    "# for i in range(1, batches+1):\n",
    "#     print(\"Generating Test Embeddings for Batch:\",i,\"of\", batches)\n",
    "#     batchEmbeddings = GetEmbeddings(X_test[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "#     all_embeddings_test.append(batchEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "#all_embeddings_test = np.concatenate(all_embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../binary/bert_embeddings.npy', all_embeddings)\n",
    "#np.save('../binary/y.npy', y)\n",
    "#np.save('../binary/bert_embeddings_test.npy', all_embeddings_test)\n",
    "#np.save('../binary/y_test.npy', y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.optimizers import adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('../binary/bert_embeddings.npy')\n",
    "y = np.load('../binary/y.npy')\n",
    "all_embeddings_test = np.load('../binary/bert_embeddings_test.npy')\n",
    "y_oh = np.load('../binary/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd = sgd(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optim = adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    embedding = Input(shape=(768,), dtype=\"float\")\n",
    "    dense1 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)\n",
    "    dense2 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense1)\n",
    "    dense3 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense2)\n",
    "    dense4 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense3)\n",
    "    dense5 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense4)\n",
    "    dense6 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense5)\n",
    "    dense7 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense6)\n",
    "    dense8 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense7)\n",
    "    dense9 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense8)\n",
    "    dense10 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense9)\n",
    "    pred = Dense(3, activation='sigmoid')(dense9)\n",
    "    model = Model(inputs=[embedding], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'], )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1000)              769000    \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 8,780,003\n",
      "Trainable params: 8,780,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=50)\n",
    "cb_list = [es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 161135 samples, validate on 162 samples\n",
      "Epoch 1/1000\n",
      "161135/161135 [==============================] - 4s 27us/step - loss: 7.6271 - acc: 0.5801 - val_loss: 5.2294 - val_acc: 0.5679\n",
      "Epoch 2/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 4.0670 - acc: 0.6040 - val_loss: 3.0244 - val_acc: 0.5679\n",
      "Epoch 3/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 2.5736 - acc: 0.6040 - val_loss: 2.1965 - val_acc: 0.5679\n",
      "Epoch 4/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.9746 - acc: 0.6040 - val_loss: 1.8250 - val_acc: 0.5679\n",
      "Epoch 5/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.7023 - acc: 0.6331 - val_loss: 1.6136 - val_acc: 0.6790\n",
      "Epoch 6/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.5496 - acc: 0.6512 - val_loss: 1.4948 - val_acc: 0.6358\n",
      "Epoch 7/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.4170 - acc: 0.6643 - val_loss: 1.3394 - val_acc: 0.6975\n",
      "Epoch 8/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.3757 - acc: 0.6448 - val_loss: 1.3411 - val_acc: 0.6481\n",
      "Epoch 9/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.2695 - acc: 0.6637 - val_loss: 1.2082 - val_acc: 0.6790\n",
      "Epoch 10/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.1919 - acc: 0.6724 - val_loss: 1.1438 - val_acc: 0.6975\n",
      "Epoch 11/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.1395 - acc: 0.6729 - val_loss: 1.1201 - val_acc: 0.6728\n",
      "Epoch 12/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.0939 - acc: 0.6745 - val_loss: 1.0431 - val_acc: 0.7037\n",
      "Epoch 13/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.0612 - acc: 0.6738 - val_loss: 1.0200 - val_acc: 0.6914\n",
      "Epoch 14/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.0193 - acc: 0.6786 - val_loss: 0.9787 - val_acc: 0.7222\n",
      "Epoch 15/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 1.0109 - acc: 0.6705 - val_loss: 0.9894 - val_acc: 0.6975\n",
      "Epoch 16/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.9820 - acc: 0.6751 - val_loss: 0.9350 - val_acc: 0.7222\n",
      "Epoch 17/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.9434 - acc: 0.6836 - val_loss: 0.9476 - val_acc: 0.7099\n",
      "Epoch 18/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.9346 - acc: 0.6797 - val_loss: 0.8970 - val_acc: 0.7222\n",
      "Epoch 19/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.9124 - acc: 0.6832 - val_loss: 0.8931 - val_acc: 0.6975\n",
      "Epoch 20/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8922 - acc: 0.6856 - val_loss: 0.8814 - val_acc: 0.7099\n",
      "Epoch 21/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8766 - acc: 0.6878 - val_loss: 0.8584 - val_acc: 0.7037\n",
      "Epoch 22/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.9397 - acc: 0.6543 - val_loss: 0.8998 - val_acc: 0.6852\n",
      "Epoch 23/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8887 - acc: 0.6758 - val_loss: 0.8617 - val_acc: 0.7222\n",
      "Epoch 24/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8589 - acc: 0.6853 - val_loss: 0.8365 - val_acc: 0.7222\n",
      "Epoch 25/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8539 - acc: 0.6835 - val_loss: 0.8242 - val_acc: 0.7099\n",
      "Epoch 26/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8372 - acc: 0.6881 - val_loss: 0.8358 - val_acc: 0.7222\n",
      "Epoch 27/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8334 - acc: 0.6876 - val_loss: 0.8081 - val_acc: 0.7160\n",
      "Epoch 28/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8242 - acc: 0.6896 - val_loss: 0.8258 - val_acc: 0.7099\n",
      "Epoch 29/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8237 - acc: 0.6882 - val_loss: 0.8014 - val_acc: 0.7160\n",
      "Epoch 30/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8156 - acc: 0.6890 - val_loss: 0.7892 - val_acc: 0.7222\n",
      "Epoch 31/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8069 - acc: 0.6922 - val_loss: 0.7783 - val_acc: 0.7222\n",
      "Epoch 32/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8028 - acc: 0.6921 - val_loss: 0.7843 - val_acc: 0.7222\n",
      "Epoch 33/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7968 - acc: 0.6942 - val_loss: 0.8060 - val_acc: 0.6914\n",
      "Epoch 34/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8025 - acc: 0.6907 - val_loss: 0.7938 - val_acc: 0.6975\n",
      "Epoch 35/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7918 - acc: 0.6936 - val_loss: 0.7752 - val_acc: 0.7284\n",
      "Epoch 36/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7773 - acc: 0.6997 - val_loss: 0.7819 - val_acc: 0.7099\n",
      "Epoch 37/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8565 - acc: 0.6635 - val_loss: 0.8226 - val_acc: 0.6914\n",
      "Epoch 38/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.8052 - acc: 0.6876 - val_loss: 0.7676 - val_acc: 0.7160\n",
      "Epoch 39/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7807 - acc: 0.6973 - val_loss: 0.7687 - val_acc: 0.7222\n",
      "Epoch 40/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7835 - acc: 0.6950 - val_loss: 0.7617 - val_acc: 0.7222\n",
      "Epoch 41/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7736 - acc: 0.6993 - val_loss: 0.7682 - val_acc: 0.7037\n",
      "Epoch 42/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7722 - acc: 0.6987 - val_loss: 0.7766 - val_acc: 0.7037\n",
      "Epoch 43/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7747 - acc: 0.6965 - val_loss: 0.8032 - val_acc: 0.6975\n",
      "Epoch 44/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7755 - acc: 0.6974 - val_loss: 0.7514 - val_acc: 0.6975\n",
      "Epoch 45/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7635 - acc: 0.7016 - val_loss: 0.7960 - val_acc: 0.7037\n",
      "Epoch 46/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7799 - acc: 0.6926 - val_loss: 0.7591 - val_acc: 0.7037\n",
      "Epoch 47/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7547 - acc: 0.7054 - val_loss: 0.7356 - val_acc: 0.7160\n",
      "Epoch 48/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7722 - acc: 0.6965 - val_loss: 0.7427 - val_acc: 0.7284\n",
      "Epoch 49/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7580 - acc: 0.7028 - val_loss: 0.7459 - val_acc: 0.7099\n",
      "Epoch 50/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7544 - acc: 0.7041 - val_loss: 0.7706 - val_acc: 0.7099\n",
      "Epoch 51/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7629 - acc: 0.7011 - val_loss: 0.7364 - val_acc: 0.7037\n",
      "Epoch 52/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7496 - acc: 0.7061 - val_loss: 0.7521 - val_acc: 0.7099\n",
      "Epoch 53/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7528 - acc: 0.7036 - val_loss: 0.7811 - val_acc: 0.6852\n",
      "Epoch 54/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7634 - acc: 0.6991 - val_loss: 0.7516 - val_acc: 0.7099\n",
      "Epoch 55/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7406 - acc: 0.7099 - val_loss: 0.7335 - val_acc: 0.7222\n",
      "Epoch 56/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7666 - acc: 0.6993 - val_loss: 0.7508 - val_acc: 0.6914\n",
      "Epoch 57/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7398 - acc: 0.7099 - val_loss: 0.7543 - val_acc: 0.7099\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7621 - acc: 0.7005 - val_loss: 0.7397 - val_acc: 0.7160\n",
      "Epoch 59/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7402 - acc: 0.7091 - val_loss: 0.7307 - val_acc: 0.7037\n",
      "Epoch 60/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7415 - acc: 0.7091 - val_loss: 0.7465 - val_acc: 0.6914\n",
      "Epoch 61/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7345 - acc: 0.7122 - val_loss: 0.7237 - val_acc: 0.7037\n",
      "Epoch 62/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7400 - acc: 0.7096 - val_loss: 0.8258 - val_acc: 0.6790\n",
      "Epoch 63/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7432 - acc: 0.7080 - val_loss: 0.7218 - val_acc: 0.7160\n",
      "Epoch 64/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7341 - acc: 0.7119 - val_loss: 0.7629 - val_acc: 0.6852\n",
      "Epoch 65/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7360 - acc: 0.7104 - val_loss: 0.7486 - val_acc: 0.6975\n",
      "Epoch 66/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7245 - acc: 0.7157 - val_loss: 0.7548 - val_acc: 0.7099\n",
      "Epoch 67/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7347 - acc: 0.7116 - val_loss: 0.7250 - val_acc: 0.7160\n",
      "Epoch 68/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7487 - acc: 0.7045 - val_loss: 0.7417 - val_acc: 0.6975\n",
      "Epoch 69/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7238 - acc: 0.7163 - val_loss: 0.7578 - val_acc: 0.7037\n",
      "Epoch 70/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7264 - acc: 0.7154 - val_loss: 0.7374 - val_acc: 0.7284\n",
      "Epoch 71/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.7411 - acc: 0.7084 - val_loss: 0.7317 - val_acc: 0.7037\n",
      "Epoch 72/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7216 - acc: 0.7174 - val_loss: 0.7181 - val_acc: 0.7160\n",
      "Epoch 73/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7303 - acc: 0.7144 - val_loss: 0.8206 - val_acc: 0.6728\n",
      "Epoch 74/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7471 - acc: 0.7041 - val_loss: 0.7563 - val_acc: 0.7160\n",
      "Epoch 75/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7240 - acc: 0.7160 - val_loss: 0.7224 - val_acc: 0.7160\n",
      "Epoch 76/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7150 - acc: 0.7201 - val_loss: 0.7125 - val_acc: 0.7099\n",
      "Epoch 77/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7355 - acc: 0.7104 - val_loss: 0.7178 - val_acc: 0.7160\n",
      "Epoch 78/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7117 - acc: 0.7209 - val_loss: 0.7268 - val_acc: 0.7160\n",
      "Epoch 79/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7353 - acc: 0.7119 - val_loss: 0.7219 - val_acc: 0.7099\n",
      "Epoch 80/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.7127 - acc: 0.7211 - val_loss: 0.7512 - val_acc: 0.7284\n",
      "Epoch 81/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.7294 - acc: 0.7144 - val_loss: 0.7191 - val_acc: 0.7284\n",
      "Epoch 82/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7047 - acc: 0.7250 - val_loss: 0.7350 - val_acc: 0.6728\n",
      "Epoch 83/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7515 - acc: 0.7032 - val_loss: 0.7761 - val_acc: 0.6914\n",
      "Epoch 84/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7228 - acc: 0.7165 - val_loss: 0.7177 - val_acc: 0.7099\n",
      "Epoch 85/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7015 - acc: 0.7269 - val_loss: 0.7137 - val_acc: 0.7099\n",
      "Epoch 86/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7148 - acc: 0.7203 - val_loss: 0.7376 - val_acc: 0.7099\n",
      "Epoch 87/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7107 - acc: 0.7228 - val_loss: 0.7220 - val_acc: 0.7284\n",
      "Epoch 88/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7314 - acc: 0.7126 - val_loss: 0.7087 - val_acc: 0.7284\n",
      "Epoch 89/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7004 - acc: 0.7269 - val_loss: 0.7794 - val_acc: 0.6975\n",
      "Epoch 90/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7273 - acc: 0.7144 - val_loss: 0.7158 - val_acc: 0.7284\n",
      "Epoch 91/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7072 - acc: 0.7242 - val_loss: 0.7244 - val_acc: 0.6975\n",
      "Epoch 92/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6898 - acc: 0.7322 - val_loss: 0.7631 - val_acc: 0.7099\n",
      "Epoch 93/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7361 - acc: 0.7073 - val_loss: 0.7292 - val_acc: 0.7222\n",
      "Epoch 94/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7044 - acc: 0.7254 - val_loss: 0.7244 - val_acc: 0.6914\n",
      "Epoch 95/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.7350 - acc: 0.7118 - val_loss: 0.7903 - val_acc: 0.6914\n",
      "Epoch 96/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7163 - acc: 0.7191 - val_loss: 0.7094 - val_acc: 0.7160\n",
      "Epoch 97/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7069 - acc: 0.7243 - val_loss: 0.7144 - val_acc: 0.7222\n",
      "Epoch 98/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6957 - acc: 0.7296 - val_loss: 0.7181 - val_acc: 0.7099\n",
      "Epoch 99/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7026 - acc: 0.7255 - val_loss: 0.7315 - val_acc: 0.7284\n",
      "Epoch 100/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7005 - acc: 0.7265 - val_loss: 0.7042 - val_acc: 0.7222\n",
      "Epoch 101/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6976 - acc: 0.7290 - val_loss: 0.7086 - val_acc: 0.6975\n",
      "Epoch 102/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7239 - acc: 0.7169 - val_loss: 0.7522 - val_acc: 0.7160\n",
      "Epoch 103/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7048 - acc: 0.7242 - val_loss: 0.7250 - val_acc: 0.7037\n",
      "Epoch 104/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6969 - acc: 0.7295 - val_loss: 0.7180 - val_acc: 0.7160\n",
      "Epoch 105/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6951 - acc: 0.7294 - val_loss: 0.7060 - val_acc: 0.7222\n",
      "Epoch 106/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6991 - acc: 0.7276 - val_loss: 0.7032 - val_acc: 0.7037\n",
      "Epoch 107/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6867 - acc: 0.7339 - val_loss: 0.7086 - val_acc: 0.7222\n",
      "Epoch 108/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7048 - acc: 0.7240 - val_loss: 0.7242 - val_acc: 0.7469\n",
      "Epoch 109/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6967 - acc: 0.7299 - val_loss: 0.7062 - val_acc: 0.7099\n",
      "Epoch 110/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7055 - acc: 0.7254 - val_loss: 0.7630 - val_acc: 0.7037\n",
      "Epoch 111/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7228 - acc: 0.7162 - val_loss: 0.7392 - val_acc: 0.7284\n",
      "Epoch 112/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6973 - acc: 0.7286 - val_loss: 0.6911 - val_acc: 0.7284\n",
      "Epoch 113/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6859 - acc: 0.7349 - val_loss: 0.7048 - val_acc: 0.7469\n",
      "Epoch 114/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6796 - acc: 0.7372 - val_loss: 0.7078 - val_acc: 0.6975\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6902 - acc: 0.7325 - val_loss: 0.7197 - val_acc: 0.7160\n",
      "Epoch 116/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6798 - acc: 0.7370 - val_loss: 0.6912 - val_acc: 0.7160\n",
      "Epoch 117/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7101 - acc: 0.7228 - val_loss: 0.7319 - val_acc: 0.7222\n",
      "Epoch 118/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7047 - acc: 0.7244 - val_loss: 0.6982 - val_acc: 0.7346\n",
      "Epoch 119/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7103 - acc: 0.7215 - val_loss: 0.7093 - val_acc: 0.7407\n",
      "Epoch 120/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6831 - acc: 0.7366 - val_loss: 0.7151 - val_acc: 0.7037\n",
      "Epoch 121/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6892 - acc: 0.7345 - val_loss: 0.7033 - val_acc: 0.7531\n",
      "Epoch 122/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6735 - acc: 0.7416 - val_loss: 0.7244 - val_acc: 0.7222\n",
      "Epoch 123/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6950 - acc: 0.7318 - val_loss: 0.7195 - val_acc: 0.6852\n",
      "Epoch 124/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7263 - acc: 0.7141 - val_loss: 0.7253 - val_acc: 0.7222\n",
      "Epoch 125/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6781 - acc: 0.7393 - val_loss: 0.7272 - val_acc: 0.7099\n",
      "Epoch 126/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6702 - acc: 0.7438 - val_loss: 0.7322 - val_acc: 0.7037\n",
      "Epoch 127/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7002 - acc: 0.7277 - val_loss: 0.7643 - val_acc: 0.7160\n",
      "Epoch 128/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6966 - acc: 0.7304 - val_loss: 0.7218 - val_acc: 0.7222\n",
      "Epoch 129/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7026 - acc: 0.7257 - val_loss: 0.7086 - val_acc: 0.7222\n",
      "Epoch 130/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6689 - acc: 0.7433 - val_loss: 0.7148 - val_acc: 0.7222\n",
      "Epoch 131/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6978 - acc: 0.7289 - val_loss: 0.7579 - val_acc: 0.7037\n",
      "Epoch 132/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6947 - acc: 0.7293 - val_loss: 0.7080 - val_acc: 0.7099\n",
      "Epoch 133/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6775 - acc: 0.7392 - val_loss: 0.7148 - val_acc: 0.7346\n",
      "Epoch 134/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6701 - acc: 0.7433 - val_loss: 0.7111 - val_acc: 0.7160\n",
      "Epoch 135/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6877 - acc: 0.7346 - val_loss: 0.7864 - val_acc: 0.6914\n",
      "Epoch 136/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7061 - acc: 0.7239 - val_loss: 0.7024 - val_acc: 0.7222\n",
      "Epoch 137/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6683 - acc: 0.7443 - val_loss: 0.6953 - val_acc: 0.7160\n",
      "Epoch 138/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6861 - acc: 0.7365 - val_loss: 0.7470 - val_acc: 0.7099\n",
      "Epoch 139/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6934 - acc: 0.7297 - val_loss: 0.7376 - val_acc: 0.7037\n",
      "Epoch 140/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.6768 - acc: 0.7396 - val_loss: 0.7313 - val_acc: 0.6790\n",
      "Epoch 141/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6684 - acc: 0.7432 - val_loss: 0.6941 - val_acc: 0.7222\n",
      "Epoch 142/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7055 - acc: 0.7252 - val_loss: 0.7234 - val_acc: 0.7160\n",
      "Epoch 143/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.7457 - val_loss: 0.7077 - val_acc: 0.7037\n",
      "Epoch 144/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6754 - acc: 0.7398 - val_loss: 0.6999 - val_acc: 0.7160\n",
      "Epoch 145/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.7483 - val_loss: 0.7874 - val_acc: 0.7037\n",
      "Epoch 146/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7057 - acc: 0.7231 - val_loss: 0.7159 - val_acc: 0.7099\n",
      "Epoch 147/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6703 - acc: 0.7417 - val_loss: 0.7579 - val_acc: 0.7222\n",
      "Epoch 148/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6802 - acc: 0.7390 - val_loss: 0.7090 - val_acc: 0.7099\n",
      "Epoch 149/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6846 - acc: 0.7362 - val_loss: 0.6904 - val_acc: 0.7222\n",
      "Epoch 150/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6650 - acc: 0.7461 - val_loss: 0.6980 - val_acc: 0.7346\n",
      "Epoch 151/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6766 - acc: 0.7393 - val_loss: 0.7156 - val_acc: 0.7160\n",
      "Epoch 152/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6643 - acc: 0.7458 - val_loss: 0.6824 - val_acc: 0.7346\n",
      "Epoch 153/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7194 - acc: 0.7186 - val_loss: 0.7013 - val_acc: 0.7099\n",
      "Epoch 154/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6665 - acc: 0.7453 - val_loss: 0.7153 - val_acc: 0.7037\n",
      "Epoch 155/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.7486 - val_loss: 0.7689 - val_acc: 0.6975\n",
      "Epoch 156/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6684 - acc: 0.7452 - val_loss: 0.7123 - val_acc: 0.7099\n",
      "Epoch 157/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6607 - acc: 0.7485 - val_loss: 0.7084 - val_acc: 0.7099\n",
      "Epoch 158/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6991 - acc: 0.7298 - val_loss: 0.6959 - val_acc: 0.7407\n",
      "Epoch 159/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.7444 - val_loss: 0.6829 - val_acc: 0.7407\n",
      "Epoch 160/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6674 - acc: 0.7456 - val_loss: 0.7255 - val_acc: 0.7160\n",
      "Epoch 161/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6629 - acc: 0.7476 - val_loss: 0.7108 - val_acc: 0.7222\n",
      "Epoch 162/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6550 - acc: 0.7520 - val_loss: 0.6901 - val_acc: 0.7407\n",
      "Epoch 163/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6730 - acc: 0.7423 - val_loss: 0.7488 - val_acc: 0.7037\n",
      "Epoch 164/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6983 - acc: 0.7291 - val_loss: 0.7444 - val_acc: 0.6975\n",
      "Epoch 165/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6668 - acc: 0.7443 - val_loss: 0.6961 - val_acc: 0.7284\n",
      "Epoch 166/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.7534 - val_loss: 0.7132 - val_acc: 0.7099\n",
      "Epoch 167/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6993 - acc: 0.7290 - val_loss: 0.6883 - val_acc: 0.7284\n",
      "Epoch 168/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6873 - acc: 0.7343 - val_loss: 0.6839 - val_acc: 0.7654\n",
      "Epoch 169/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6671 - acc: 0.7457 - val_loss: 0.6885 - val_acc: 0.7037\n",
      "Epoch 170/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6454 - acc: 0.7563 - val_loss: 0.7121 - val_acc: 0.7037\n",
      "Epoch 171/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6723 - acc: 0.7419 - val_loss: 0.7001 - val_acc: 0.7284\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.7535 - val_loss: 0.6978 - val_acc: 0.7222\n",
      "Epoch 173/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6664 - acc: 0.7455 - val_loss: 0.6916 - val_acc: 0.7346\n",
      "Epoch 174/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.7495 - val_loss: 0.7059 - val_acc: 0.6667\n",
      "Epoch 175/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6586 - acc: 0.7494 - val_loss: 0.6912 - val_acc: 0.7346\n",
      "Epoch 176/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.7543 - val_loss: 0.7012 - val_acc: 0.7160\n",
      "Epoch 177/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6627 - acc: 0.7480 - val_loss: 0.7032 - val_acc: 0.7346\n",
      "Epoch 178/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.7591 - val_loss: 0.7517 - val_acc: 0.6975\n",
      "Epoch 179/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6995 - acc: 0.7258 - val_loss: 0.7196 - val_acc: 0.7222\n",
      "Epoch 180/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6715 - acc: 0.7430 - val_loss: 0.6775 - val_acc: 0.7469\n",
      "Epoch 181/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6519 - acc: 0.7536 - val_loss: 0.7050 - val_acc: 0.7037\n",
      "Epoch 182/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.7532 - val_loss: 0.7016 - val_acc: 0.7469\n",
      "Epoch 183/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6996 - acc: 0.7276 - val_loss: 0.7236 - val_acc: 0.7160\n",
      "Epoch 184/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6807 - acc: 0.7391 - val_loss: 0.7207 - val_acc: 0.7037\n",
      "Epoch 185/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6747 - acc: 0.7403 - val_loss: 0.7270 - val_acc: 0.7099\n",
      "Epoch 186/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6619 - acc: 0.7478 - val_loss: 0.7075 - val_acc: 0.7346\n",
      "Epoch 187/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.7497 - val_loss: 0.6747 - val_acc: 0.7284\n",
      "Epoch 188/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6774 - acc: 0.7382 - val_loss: 0.6826 - val_acc: 0.7407\n",
      "Epoch 189/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.7503 - val_loss: 0.6830 - val_acc: 0.7222\n",
      "Epoch 190/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6393 - acc: 0.7611 - val_loss: 0.6852 - val_acc: 0.7160\n",
      "Epoch 191/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6409 - acc: 0.7601 - val_loss: 0.7282 - val_acc: 0.6975\n",
      "Epoch 192/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6903 - acc: 0.7330 - val_loss: 0.7135 - val_acc: 0.7099\n",
      "Epoch 193/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6610 - acc: 0.7465 - val_loss: 0.7119 - val_acc: 0.7222\n",
      "Epoch 194/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6445 - acc: 0.7577 - val_loss: 0.7150 - val_acc: 0.6975\n",
      "Epoch 195/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6380 - acc: 0.7617 - val_loss: 0.6971 - val_acc: 0.7407\n",
      "Epoch 196/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6327 - acc: 0.7645 - val_loss: 0.6968 - val_acc: 0.6914\n",
      "Epoch 197/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6763 - acc: 0.7423 - val_loss: 0.7900 - val_acc: 0.6914\n",
      "Epoch 198/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6908 - acc: 0.7316 - val_loss: 0.6878 - val_acc: 0.7407\n",
      "Epoch 199/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6422 - acc: 0.7592 - val_loss: 0.7644 - val_acc: 0.7160\n",
      "Epoch 200/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.7513 - val_loss: 0.7106 - val_acc: 0.7037\n",
      "Epoch 201/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6417 - acc: 0.7588 - val_loss: 0.6982 - val_acc: 0.7099\n",
      "Epoch 202/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6473 - acc: 0.7572 - val_loss: 0.7225 - val_acc: 0.7099\n",
      "Epoch 203/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.7488 - val_loss: 0.7677 - val_acc: 0.7099\n",
      "Epoch 204/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6826 - acc: 0.7359 - val_loss: 0.7213 - val_acc: 0.7222\n",
      "Epoch 205/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.7469 - val_loss: 0.7256 - val_acc: 0.7160\n",
      "Epoch 206/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6405 - acc: 0.7597 - val_loss: 0.6926 - val_acc: 0.7284\n",
      "Epoch 207/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6356 - acc: 0.7629 - val_loss: 0.7194 - val_acc: 0.6914\n",
      "Epoch 208/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.6499 - acc: 0.7552 - val_loss: 0.7420 - val_acc: 0.7160\n",
      "Epoch 209/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6841 - acc: 0.7353 - val_loss: 0.6873 - val_acc: 0.7407\n",
      "Epoch 210/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.7479 - val_loss: 0.7103 - val_acc: 0.7099\n",
      "Epoch 211/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6351 - acc: 0.7628 - val_loss: 0.7113 - val_acc: 0.6914\n",
      "Epoch 212/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6232 - acc: 0.7705 - val_loss: 0.6841 - val_acc: 0.7160\n",
      "Epoch 213/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6571 - acc: 0.7532 - val_loss: 0.7955 - val_acc: 0.7037\n",
      "Epoch 214/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.7489 - val_loss: 0.7498 - val_acc: 0.7160\n",
      "Epoch 215/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.7593 - val_loss: 0.7066 - val_acc: 0.7160\n",
      "Epoch 216/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.7587 - val_loss: 0.7045 - val_acc: 0.7037\n",
      "Epoch 217/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6336 - acc: 0.7642 - val_loss: 0.7812 - val_acc: 0.6728\n",
      "Epoch 218/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6675 - acc: 0.7462 - val_loss: 0.7504 - val_acc: 0.7037\n",
      "Epoch 219/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.7474 - val_loss: 0.6857 - val_acc: 0.7407\n",
      "Epoch 220/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6327 - acc: 0.7638 - val_loss: 0.6993 - val_acc: 0.7099\n",
      "Epoch 221/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6275 - acc: 0.7675 - val_loss: 0.7614 - val_acc: 0.6852\n",
      "Epoch 222/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.7556 - val_loss: 0.6925 - val_acc: 0.7407\n",
      "Epoch 223/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6317 - acc: 0.7647 - val_loss: 0.7094 - val_acc: 0.7346\n",
      "Epoch 224/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6907 - acc: 0.7313 - val_loss: 0.7929 - val_acc: 0.6667\n",
      "Epoch 225/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6608 - acc: 0.7504 - val_loss: 0.7390 - val_acc: 0.7160\n",
      "Epoch 226/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6252 - acc: 0.7718 - val_loss: 0.7235 - val_acc: 0.7346\n",
      "Epoch 227/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6428 - acc: 0.7586 - val_loss: 0.7295 - val_acc: 0.6975\n",
      "Epoch 228/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6388 - acc: 0.7607 - val_loss: 0.7357 - val_acc: 0.6728\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6678 - acc: 0.7464 - val_loss: 0.7394 - val_acc: 0.6914\n",
      "Epoch 230/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6573 - acc: 0.7508 - val_loss: 0.7131 - val_acc: 0.7160\n",
      "Epoch 231/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6374 - acc: 0.7607 - val_loss: 0.6915 - val_acc: 0.7160\n",
      "Epoch 232/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.7610 - val_loss: 0.7467 - val_acc: 0.7099\n",
      "Epoch 233/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6834 - acc: 0.7386 - val_loss: 0.7043 - val_acc: 0.7346\n",
      "Epoch 234/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6385 - acc: 0.7614 - val_loss: 0.7196 - val_acc: 0.7099\n",
      "Epoch 235/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6294 - acc: 0.7671 - val_loss: 0.7162 - val_acc: 0.7099\n",
      "Epoch 236/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6253 - acc: 0.7705 - val_loss: 0.7661 - val_acc: 0.6852\n",
      "Epoch 237/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6325 - acc: 0.7658 - val_loss: 0.6892 - val_acc: 0.7099\n",
      "Epoch 238/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6473 - acc: 0.7583 - val_loss: 0.7000 - val_acc: 0.7284\n",
      "Epoch 239/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6862 - acc: 0.7335 - val_loss: 0.6867 - val_acc: 0.7531\n",
      "Epoch 240/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.7547 - val_loss: 0.6864 - val_acc: 0.7222\n",
      "Epoch 241/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6431 - acc: 0.7577 - val_loss: 0.7144 - val_acc: 0.7346\n",
      "Epoch 242/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6241 - acc: 0.7692 - val_loss: 0.7059 - val_acc: 0.7407\n",
      "Epoch 243/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6267 - acc: 0.7686 - val_loss: 0.7147 - val_acc: 0.7099\n",
      "Epoch 244/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6333 - acc: 0.7649 - val_loss: 0.7148 - val_acc: 0.7160\n",
      "Epoch 245/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7087 - acc: 0.7252 - val_loss: 0.7594 - val_acc: 0.6975\n",
      "Epoch 246/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6784 - acc: 0.7419 - val_loss: 0.6924 - val_acc: 0.7469\n",
      "Epoch 247/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6429 - acc: 0.7584 - val_loss: 0.7116 - val_acc: 0.7160\n",
      "Epoch 248/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6365 - acc: 0.7631 - val_loss: 0.7019 - val_acc: 0.7037\n",
      "Epoch 249/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6172 - acc: 0.7734 - val_loss: 0.7080 - val_acc: 0.7222\n",
      "Epoch 250/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6148 - acc: 0.7743 - val_loss: 0.6946 - val_acc: 0.7222\n",
      "Epoch 251/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6690 - acc: 0.7434 - val_loss: 0.7094 - val_acc: 0.7346\n",
      "Epoch 252/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6787 - acc: 0.7409 - val_loss: 0.6731 - val_acc: 0.7531\n",
      "Epoch 253/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6226 - acc: 0.7708 - val_loss: 0.7309 - val_acc: 0.7346\n",
      "Epoch 254/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6334 - acc: 0.7638 - val_loss: 0.7072 - val_acc: 0.7160\n",
      "Epoch 255/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6774 - acc: 0.7394 - val_loss: 0.7268 - val_acc: 0.6975\n",
      "Epoch 256/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.7554 - val_loss: 0.6881 - val_acc: 0.7160\n",
      "Epoch 257/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6223 - acc: 0.7703 - val_loss: 0.7278 - val_acc: 0.7099\n",
      "Epoch 258/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6331 - acc: 0.7654 - val_loss: 0.6840 - val_acc: 0.7531\n",
      "Epoch 259/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6132 - acc: 0.7746 - val_loss: 0.7048 - val_acc: 0.7037\n",
      "Epoch 260/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6204 - acc: 0.7716 - val_loss: 0.7135 - val_acc: 0.7469\n",
      "Epoch 261/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6686 - acc: 0.7454 - val_loss: 0.6772 - val_acc: 0.7284\n",
      "Epoch 262/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6149 - acc: 0.7745 - val_loss: 0.7178 - val_acc: 0.7160\n",
      "Epoch 263/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6386 - acc: 0.7623 - val_loss: 0.6831 - val_acc: 0.7284\n",
      "Epoch 264/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6210 - acc: 0.7708 - val_loss: 0.6938 - val_acc: 0.7099\n",
      "Epoch 265/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6357 - acc: 0.7639 - val_loss: 0.6736 - val_acc: 0.7407\n",
      "Epoch 266/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6201 - acc: 0.7727 - val_loss: 0.7346 - val_acc: 0.7160\n",
      "Epoch 267/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6232 - acc: 0.7705 - val_loss: 0.6817 - val_acc: 0.7469\n",
      "Epoch 268/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6316 - acc: 0.7667 - val_loss: 0.7092 - val_acc: 0.7222\n",
      "Epoch 269/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6257 - acc: 0.7682 - val_loss: 0.6918 - val_acc: 0.7593\n",
      "Epoch 270/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6661 - acc: 0.7487 - val_loss: 0.7670 - val_acc: 0.6914\n",
      "Epoch 271/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6615 - acc: 0.7486 - val_loss: 0.7263 - val_acc: 0.7160\n",
      "Epoch 272/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6356 - acc: 0.7620 - val_loss: 0.6980 - val_acc: 0.7346\n",
      "Epoch 273/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6056 - acc: 0.7807 - val_loss: 0.6778 - val_acc: 0.7593\n",
      "Epoch 274/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6373 - acc: 0.7645 - val_loss: 0.7102 - val_acc: 0.7222\n",
      "Epoch 275/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.7475 - val_loss: 0.7427 - val_acc: 0.7160\n",
      "Epoch 276/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.7542 - val_loss: 0.6684 - val_acc: 0.7160\n",
      "Epoch 277/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6125 - acc: 0.7764 - val_loss: 0.7507 - val_acc: 0.7407\n",
      "Epoch 278/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6249 - acc: 0.7701 - val_loss: 0.7085 - val_acc: 0.7222\n",
      "Epoch 279/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6208 - acc: 0.7712 - val_loss: 0.6861 - val_acc: 0.7284\n",
      "Epoch 280/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6650 - acc: 0.7472 - val_loss: 0.7508 - val_acc: 0.7037\n",
      "Epoch 281/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.7503 - val_loss: 0.6870 - val_acc: 0.7099\n",
      "Epoch 282/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.6143 - acc: 0.7753 - val_loss: 0.6930 - val_acc: 0.7346\n",
      "Epoch 283/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6255 - acc: 0.7702 - val_loss: 0.7091 - val_acc: 0.7160\n",
      "Epoch 284/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6229 - acc: 0.7705 - val_loss: 0.6998 - val_acc: 0.6975\n",
      "Epoch 285/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6081 - acc: 0.7798 - val_loss: 0.6840 - val_acc: 0.7407\n",
      "Epoch 286/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6445 - acc: 0.7581 - val_loss: 0.7635 - val_acc: 0.6914\n",
      "Epoch 287/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6638 - acc: 0.7457 - val_loss: 0.7171 - val_acc: 0.6975\n",
      "Epoch 288/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6368 - acc: 0.7629 - val_loss: 0.6768 - val_acc: 0.7531\n",
      "Epoch 289/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6011 - acc: 0.7849 - val_loss: 0.6907 - val_acc: 0.7407\n",
      "Epoch 290/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6094 - acc: 0.7786 - val_loss: 0.6904 - val_acc: 0.7407\n",
      "Epoch 291/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6230 - acc: 0.7710 - val_loss: 0.6730 - val_acc: 0.7407\n",
      "Epoch 292/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6222 - acc: 0.7716 - val_loss: 0.6854 - val_acc: 0.7222\n",
      "Epoch 293/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6316 - acc: 0.7658 - val_loss: 0.7464 - val_acc: 0.7222\n",
      "Epoch 294/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7159 - acc: 0.7239 - val_loss: 0.7460 - val_acc: 0.7099\n",
      "Epoch 295/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.7561 - val_loss: 0.7086 - val_acc: 0.7284\n",
      "Epoch 296/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6086 - acc: 0.7815 - val_loss: 0.7054 - val_acc: 0.7099\n",
      "Epoch 297/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6170 - acc: 0.7758 - val_loss: 0.6942 - val_acc: 0.7284\n",
      "Epoch 298/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6242 - acc: 0.7715 - val_loss: 0.6929 - val_acc: 0.7284\n",
      "Epoch 299/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6054 - acc: 0.7827 - val_loss: 0.7093 - val_acc: 0.7099\n",
      "Epoch 300/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6339 - acc: 0.7659 - val_loss: 0.6987 - val_acc: 0.7284\n",
      "Epoch 301/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6046 - acc: 0.7821 - val_loss: 0.7279 - val_acc: 0.7284\n",
      "Epoch 302/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6368 - acc: 0.7648 - val_loss: 0.7905 - val_acc: 0.6914\n",
      "Epoch 303/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6753 - acc: 0.7405 - val_loss: 0.7243 - val_acc: 0.7407\n",
      "Epoch 304/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6384 - acc: 0.7621 - val_loss: 0.6847 - val_acc: 0.7222\n",
      "Epoch 305/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6027 - acc: 0.7844 - val_loss: 0.6896 - val_acc: 0.7346\n",
      "Epoch 306/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6513 - acc: 0.7563 - val_loss: 0.7897 - val_acc: 0.6852\n",
      "Epoch 307/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.7590 - val_loss: 0.6725 - val_acc: 0.7284\n",
      "Epoch 308/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6137 - acc: 0.7769 - val_loss: 0.6917 - val_acc: 0.7160\n",
      "Epoch 309/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6190 - acc: 0.7751 - val_loss: 0.7059 - val_acc: 0.7160\n",
      "Epoch 310/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6715 - acc: 0.7450 - val_loss: 0.7180 - val_acc: 0.7593\n",
      "Epoch 311/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6372 - acc: 0.7631 - val_loss: 0.6793 - val_acc: 0.7593\n",
      "Epoch 312/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6393 - acc: 0.7627 - val_loss: 0.7737 - val_acc: 0.6975\n",
      "Epoch 313/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6724 - acc: 0.7427 - val_loss: 0.7213 - val_acc: 0.7037\n",
      "Epoch 314/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6236 - acc: 0.7702 - val_loss: 0.7107 - val_acc: 0.7469\n",
      "Epoch 315/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6377 - acc: 0.7638 - val_loss: 0.7892 - val_acc: 0.6914\n",
      "Epoch 316/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.7603 - val_loss: 0.7053 - val_acc: 0.7222\n",
      "Epoch 317/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6286 - acc: 0.7665 - val_loss: 0.7005 - val_acc: 0.7346\n",
      "Epoch 318/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6085 - acc: 0.7796 - val_loss: 0.6736 - val_acc: 0.7160\n",
      "Epoch 319/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6016 - acc: 0.7854 - val_loss: 0.6851 - val_acc: 0.7222\n",
      "Epoch 320/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6089 - acc: 0.7786 - val_loss: 0.6915 - val_acc: 0.7284\n",
      "Epoch 321/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6186 - acc: 0.7740 - val_loss: 0.6554 - val_acc: 0.7407\n",
      "Epoch 322/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.7460 - val_loss: 0.7084 - val_acc: 0.7346\n",
      "Epoch 323/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6195 - acc: 0.7733 - val_loss: 0.6842 - val_acc: 0.7407\n",
      "Epoch 324/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6097 - acc: 0.7790 - val_loss: 0.6829 - val_acc: 0.7593\n",
      "Epoch 325/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5959 - acc: 0.7884 - val_loss: 0.7213 - val_acc: 0.7346\n",
      "Epoch 326/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6203 - acc: 0.7734 - val_loss: 0.6761 - val_acc: 0.7469\n",
      "Epoch 327/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6105 - acc: 0.7785 - val_loss: 0.6862 - val_acc: 0.7407\n",
      "Epoch 328/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6160 - acc: 0.7762 - val_loss: 0.6866 - val_acc: 0.7531\n",
      "Epoch 329/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5968 - acc: 0.7871 - val_loss: 0.6709 - val_acc: 0.7284\n",
      "Epoch 330/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6235 - acc: 0.7717 - val_loss: 0.6611 - val_acc: 0.7469\n",
      "Epoch 331/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6065 - acc: 0.7810 - val_loss: 0.6967 - val_acc: 0.7346\n",
      "Epoch 332/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.7507 - val_loss: 0.7507 - val_acc: 0.7099\n",
      "Epoch 333/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.7525 - val_loss: 0.6927 - val_acc: 0.7346\n",
      "Epoch 334/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.7622 - val_loss: 0.6879 - val_acc: 0.7469\n",
      "Epoch 335/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5959 - acc: 0.7883 - val_loss: 0.6792 - val_acc: 0.7469\n",
      "Epoch 336/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6188 - acc: 0.7747 - val_loss: 0.6704 - val_acc: 0.7469\n",
      "Epoch 337/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5888 - acc: 0.7928 - val_loss: 0.6841 - val_acc: 0.7346\n",
      "Epoch 338/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6182 - acc: 0.7756 - val_loss: 0.6803 - val_acc: 0.7469\n",
      "Epoch 339/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5909 - acc: 0.7907 - val_loss: 0.6701 - val_acc: 0.7407\n",
      "Epoch 340/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6246 - acc: 0.7730 - val_loss: 0.7132 - val_acc: 0.7407\n",
      "Epoch 341/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6056 - acc: 0.7817 - val_loss: 0.7362 - val_acc: 0.7160\n",
      "Epoch 342/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6265 - acc: 0.7705 - val_loss: 0.7515 - val_acc: 0.6975\n",
      "Epoch 343/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6169 - acc: 0.7753 - val_loss: 0.6824 - val_acc: 0.7407\n",
      "Epoch 344/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6109 - acc: 0.7789 - val_loss: 0.7528 - val_acc: 0.7160\n",
      "Epoch 345/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6180 - acc: 0.7751 - val_loss: 0.7032 - val_acc: 0.7160\n",
      "Epoch 346/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6128 - acc: 0.7772 - val_loss: 0.7098 - val_acc: 0.6914\n",
      "Epoch 347/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6091 - acc: 0.7802 - val_loss: 0.7281 - val_acc: 0.6975\n",
      "Epoch 348/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6796 - acc: 0.7401 - val_loss: 0.6552 - val_acc: 0.7654\n",
      "Epoch 349/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6124 - acc: 0.7775 - val_loss: 0.6866 - val_acc: 0.7222\n",
      "Epoch 350/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6108 - acc: 0.7801 - val_loss: 0.6908 - val_acc: 0.7407\n",
      "Epoch 351/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6034 - acc: 0.7832 - val_loss: 0.6942 - val_acc: 0.7284\n",
      "Epoch 352/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6129 - acc: 0.7772 - val_loss: 0.7152 - val_acc: 0.7346\n",
      "Epoch 353/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6083 - acc: 0.7811 - val_loss: 0.6711 - val_acc: 0.7407\n",
      "Epoch 354/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5986 - acc: 0.7862 - val_loss: 0.7102 - val_acc: 0.7222\n",
      "Epoch 355/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6064 - acc: 0.7809 - val_loss: 0.7483 - val_acc: 0.6975\n",
      "Epoch 356/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6641 - acc: 0.7470 - val_loss: 0.7033 - val_acc: 0.7346\n",
      "Epoch 357/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6168 - acc: 0.7755 - val_loss: 0.6874 - val_acc: 0.7346\n",
      "Epoch 358/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5993 - acc: 0.7866 - val_loss: 0.7197 - val_acc: 0.7222\n",
      "Epoch 359/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6241 - acc: 0.7716 - val_loss: 0.6822 - val_acc: 0.7407\n",
      "Epoch 360/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6084 - acc: 0.7808 - val_loss: 0.6897 - val_acc: 0.7346\n",
      "Epoch 361/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5886 - acc: 0.7918 - val_loss: 0.6826 - val_acc: 0.7407\n",
      "Epoch 362/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6186 - acc: 0.7751 - val_loss: 0.7304 - val_acc: 0.7160\n",
      "Epoch 363/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6993 - acc: 0.7311 - val_loss: 0.6846 - val_acc: 0.7654\n",
      "Epoch 364/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6260 - acc: 0.7724 - val_loss: 0.6665 - val_acc: 0.7346\n",
      "Epoch 365/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5969 - acc: 0.7872 - val_loss: 0.6765 - val_acc: 0.7654\n",
      "Epoch 366/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.6656 - acc: 0.7495 - val_loss: 0.7851 - val_acc: 0.6728\n",
      "Epoch 367/1000\n",
      "161135/161135 [==============================] - 2s 11us/step - loss: 0.6513 - acc: 0.7557 - val_loss: 0.6733 - val_acc: 0.7778\n",
      "Epoch 368/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5943 - acc: 0.7887 - val_loss: 0.6796 - val_acc: 0.7407\n",
      "Epoch 369/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6093 - acc: 0.7802 - val_loss: 0.6805 - val_acc: 0.7469\n",
      "Epoch 370/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5874 - acc: 0.7936 - val_loss: 0.6892 - val_acc: 0.7469\n",
      "Epoch 371/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6015 - acc: 0.7840 - val_loss: 0.6719 - val_acc: 0.7469\n",
      "Epoch 372/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6335 - acc: 0.7667 - val_loss: 0.6706 - val_acc: 0.7407\n",
      "Epoch 373/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6052 - acc: 0.7816 - val_loss: 0.6569 - val_acc: 0.7654\n",
      "Epoch 374/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6010 - acc: 0.7872 - val_loss: 0.6963 - val_acc: 0.7469\n",
      "Epoch 375/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5903 - acc: 0.7917 - val_loss: 0.6414 - val_acc: 0.7840\n",
      "Epoch 376/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6176 - acc: 0.7783 - val_loss: 0.8937 - val_acc: 0.6481\n",
      "Epoch 377/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7138 - acc: 0.7224 - val_loss: 0.6802 - val_acc: 0.7407\n",
      "Epoch 378/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6155 - acc: 0.7770 - val_loss: 0.7106 - val_acc: 0.7407\n",
      "Epoch 379/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5856 - acc: 0.7958 - val_loss: 0.7180 - val_acc: 0.7037\n",
      "Epoch 380/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6087 - acc: 0.7816 - val_loss: 0.6578 - val_acc: 0.7654\n",
      "Epoch 381/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5964 - acc: 0.7873 - val_loss: 0.7093 - val_acc: 0.7037\n",
      "Epoch 382/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6044 - acc: 0.7847 - val_loss: 0.6591 - val_acc: 0.7716\n",
      "Epoch 383/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6250 - acc: 0.7696 - val_loss: 0.7260 - val_acc: 0.7160\n",
      "Epoch 384/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6242 - acc: 0.7702 - val_loss: 0.6787 - val_acc: 0.7222\n",
      "Epoch 385/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5840 - acc: 0.7956 - val_loss: 0.6840 - val_acc: 0.7346\n",
      "Epoch 386/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6303 - acc: 0.7684 - val_loss: 0.6887 - val_acc: 0.7160\n",
      "Epoch 387/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.7522 - val_loss: 0.6639 - val_acc: 0.7716\n",
      "Epoch 388/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6130 - acc: 0.7776 - val_loss: 0.6606 - val_acc: 0.7531\n",
      "Epoch 389/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6015 - acc: 0.7858 - val_loss: 0.6654 - val_acc: 0.7407\n",
      "Epoch 390/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5876 - acc: 0.7935 - val_loss: 0.7127 - val_acc: 0.7160\n",
      "Epoch 391/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6920 - acc: 0.7367 - val_loss: 0.7294 - val_acc: 0.6975\n",
      "Epoch 392/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6184 - acc: 0.7750 - val_loss: 0.6708 - val_acc: 0.7284\n",
      "Epoch 393/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5855 - acc: 0.7942 - val_loss: 0.7010 - val_acc: 0.7284\n",
      "Epoch 394/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.7551 - val_loss: 0.7109 - val_acc: 0.7284\n",
      "Epoch 395/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6166 - acc: 0.7750 - val_loss: 0.6577 - val_acc: 0.7469\n",
      "Epoch 396/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5877 - acc: 0.7937 - val_loss: 0.6654 - val_acc: 0.7654\n",
      "Epoch 397/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5898 - acc: 0.7926 - val_loss: 0.6753 - val_acc: 0.7469\n",
      "Epoch 398/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5826 - acc: 0.7963 - val_loss: 0.7173 - val_acc: 0.6975\n",
      "Epoch 399/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6657 - acc: 0.7514 - val_loss: 0.7425 - val_acc: 0.6975\n",
      "Epoch 400/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6340 - acc: 0.7671 - val_loss: 0.6634 - val_acc: 0.7716\n",
      "Epoch 401/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5913 - acc: 0.7919 - val_loss: 0.7006 - val_acc: 0.7222\n",
      "Epoch 402/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5983 - acc: 0.7872 - val_loss: 0.7121 - val_acc: 0.7346\n",
      "Epoch 403/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5918 - acc: 0.7912 - val_loss: 0.6739 - val_acc: 0.7531\n",
      "Epoch 404/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5794 - acc: 0.7985 - val_loss: 0.8180 - val_acc: 0.6605\n",
      "Epoch 405/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6821 - acc: 0.7389 - val_loss: 0.7003 - val_acc: 0.7593\n",
      "Epoch 406/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6176 - acc: 0.7768 - val_loss: 0.6766 - val_acc: 0.7469\n",
      "Epoch 407/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5881 - acc: 0.7937 - val_loss: 0.6790 - val_acc: 0.7346\n",
      "Epoch 408/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6086 - acc: 0.7814 - val_loss: 0.6796 - val_acc: 0.7346\n",
      "Epoch 409/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5777 - acc: 0.7996 - val_loss: 0.6635 - val_acc: 0.7531\n",
      "Epoch 410/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6742 - acc: 0.7447 - val_loss: 0.8087 - val_acc: 0.7037\n",
      "Epoch 411/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6351 - acc: 0.7678 - val_loss: 0.6851 - val_acc: 0.7531\n",
      "Epoch 412/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5927 - acc: 0.7895 - val_loss: 0.6997 - val_acc: 0.7346\n",
      "Epoch 413/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5954 - acc: 0.7890 - val_loss: 0.6804 - val_acc: 0.7531\n",
      "Epoch 414/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5848 - acc: 0.7946 - val_loss: 0.7514 - val_acc: 0.7284\n",
      "Epoch 415/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6805 - acc: 0.7427 - val_loss: 0.6873 - val_acc: 0.7407\n",
      "Epoch 416/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6066 - acc: 0.7824 - val_loss: 0.6513 - val_acc: 0.7716\n",
      "Epoch 417/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5810 - acc: 0.7975 - val_loss: 0.7431 - val_acc: 0.7222\n",
      "Epoch 418/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5979 - acc: 0.7883 - val_loss: 0.6715 - val_acc: 0.7407\n",
      "Epoch 419/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5979 - acc: 0.7873 - val_loss: 0.7511 - val_acc: 0.7099\n",
      "Epoch 420/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6458 - acc: 0.7599 - val_loss: 0.6825 - val_acc: 0.7469\n",
      "Epoch 421/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6201 - acc: 0.7739 - val_loss: 0.6853 - val_acc: 0.7469\n",
      "Epoch 422/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5909 - acc: 0.7921 - val_loss: 0.7248 - val_acc: 0.7160\n",
      "Epoch 423/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5841 - acc: 0.7956 - val_loss: 0.6731 - val_acc: 0.7469\n",
      "Epoch 424/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6068 - acc: 0.7828 - val_loss: 0.7078 - val_acc: 0.7531\n",
      "Epoch 425/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.7528 - val_loss: 0.6773 - val_acc: 0.7284\n",
      "Epoch 426/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6051 - acc: 0.7825 - val_loss: 0.6494 - val_acc: 0.7654\n",
      "Epoch 427/1000\n",
      "161135/161135 [==============================] - 1s 9us/step - loss: 0.6164 - acc: 0.7775 - val_loss: 0.6875 - val_acc: 0.7346\n",
      "Epoch 428/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.8135 - acc: 0.6773 - val_loss: 0.7835 - val_acc: 0.6975\n",
      "Epoch 429/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.7024 - acc: 0.7343 - val_loss: 0.6702 - val_acc: 0.7346\n",
      "Epoch 430/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6037 - acc: 0.7842 - val_loss: 0.7059 - val_acc: 0.7099\n",
      "Epoch 431/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5789 - acc: 0.8007 - val_loss: 0.6879 - val_acc: 0.7469\n",
      "Epoch 432/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5870 - acc: 0.7951 - val_loss: 0.6798 - val_acc: 0.7160\n",
      "Epoch 433/1000\n",
      "161135/161135 [==============================] - 1s 9us/step - loss: 0.6294 - acc: 0.7682 - val_loss: 0.6709 - val_acc: 0.7346\n",
      "Epoch 434/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5778 - acc: 0.7994 - val_loss: 0.6575 - val_acc: 0.7531\n",
      "Epoch 435/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6054 - acc: 0.7853 - val_loss: 0.6640 - val_acc: 0.7593\n",
      "Epoch 436/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5855 - acc: 0.7959 - val_loss: 0.6547 - val_acc: 0.7778\n",
      "Epoch 437/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5866 - acc: 0.7948 - val_loss: 0.7414 - val_acc: 0.7099\n",
      "Epoch 438/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6789 - acc: 0.7440 - val_loss: 0.6583 - val_acc: 0.7654\n",
      "Epoch 439/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6155 - acc: 0.7766 - val_loss: 0.6845 - val_acc: 0.7531\n",
      "Epoch 440/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5977 - acc: 0.7880 - val_loss: 0.6798 - val_acc: 0.7469\n",
      "Epoch 441/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5823 - acc: 0.7974 - val_loss: 0.7048 - val_acc: 0.7284\n",
      "Epoch 442/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5887 - acc: 0.7942 - val_loss: 0.6718 - val_acc: 0.7469\n",
      "Epoch 443/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5934 - acc: 0.7907 - val_loss: 0.6632 - val_acc: 0.7469\n",
      "Epoch 444/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5997 - acc: 0.7883 - val_loss: 0.7005 - val_acc: 0.7037\n",
      "Epoch 445/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5738 - acc: 0.8036 - val_loss: 0.7929 - val_acc: 0.7160\n",
      "Epoch 446/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6099 - acc: 0.7827 - val_loss: 0.6794 - val_acc: 0.7407\n",
      "Epoch 447/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5874 - acc: 0.7941 - val_loss: 0.7216 - val_acc: 0.7222\n",
      "Epoch 448/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6041 - acc: 0.7852 - val_loss: 0.7784 - val_acc: 0.7037\n",
      "Epoch 449/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6639 - acc: 0.7506 - val_loss: 0.7209 - val_acc: 0.7160\n",
      "Epoch 450/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6266 - acc: 0.7703 - val_loss: 0.6757 - val_acc: 0.7407\n",
      "Epoch 451/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5736 - acc: 0.8031 - val_loss: 0.6953 - val_acc: 0.7469\n",
      "Epoch 452/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6121 - acc: 0.7799 - val_loss: 0.8526 - val_acc: 0.6790\n",
      "Epoch 453/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6806 - acc: 0.7427 - val_loss: 0.6656 - val_acc: 0.7469\n",
      "Epoch 454/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5932 - acc: 0.7914 - val_loss: 0.6921 - val_acc: 0.7407\n",
      "Epoch 455/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5760 - acc: 0.8016 - val_loss: 0.7174 - val_acc: 0.7346\n",
      "Epoch 456/1000\n",
      "161135/161135 [==============================] - 1s 9us/step - loss: 0.5989 - acc: 0.7874 - val_loss: 0.6606 - val_acc: 0.7469\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6511 - acc: 0.7584 - val_loss: 0.6702 - val_acc: 0.7407\n",
      "Epoch 458/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5981 - acc: 0.7883 - val_loss: 0.6490 - val_acc: 0.7531\n",
      "Epoch 459/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5780 - acc: 0.8008 - val_loss: 0.7231 - val_acc: 0.7346\n",
      "Epoch 460/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5966 - acc: 0.7899 - val_loss: 0.6770 - val_acc: 0.7284\n",
      "Epoch 461/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5779 - acc: 0.8002 - val_loss: 0.6861 - val_acc: 0.7346\n",
      "Epoch 462/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6841 - acc: 0.7410 - val_loss: 0.7289 - val_acc: 0.7346\n",
      "Epoch 463/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6289 - acc: 0.7726 - val_loss: 0.6741 - val_acc: 0.7469\n",
      "Epoch 464/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5744 - acc: 0.8035 - val_loss: 0.6827 - val_acc: 0.7593\n",
      "Epoch 465/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6028 - acc: 0.7853 - val_loss: 0.7902 - val_acc: 0.6914\n",
      "Epoch 466/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6637 - acc: 0.7512 - val_loss: 0.7082 - val_acc: 0.7716\n",
      "Epoch 467/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6040 - acc: 0.7842 - val_loss: 0.6644 - val_acc: 0.7593\n",
      "Epoch 468/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5668 - acc: 0.8075 - val_loss: 0.6928 - val_acc: 0.7099\n",
      "Epoch 469/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5799 - acc: 0.7989 - val_loss: 0.7105 - val_acc: 0.7284\n",
      "Epoch 470/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5766 - acc: 0.8015 - val_loss: 0.7455 - val_acc: 0.7284\n",
      "Epoch 471/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6123 - acc: 0.7799 - val_loss: 0.7146 - val_acc: 0.7593\n",
      "Epoch 472/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6374 - acc: 0.7646 - val_loss: 0.6629 - val_acc: 0.7716\n",
      "Epoch 473/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5927 - acc: 0.7914 - val_loss: 0.6822 - val_acc: 0.7222\n",
      "Epoch 474/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5858 - acc: 0.7951 - val_loss: 0.6752 - val_acc: 0.7593\n",
      "Epoch 475/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5850 - acc: 0.7960 - val_loss: 0.6903 - val_acc: 0.7407\n",
      "Epoch 476/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5896 - acc: 0.7927 - val_loss: 0.6751 - val_acc: 0.7469\n",
      "Epoch 477/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5824 - acc: 0.7968 - val_loss: 0.6959 - val_acc: 0.7469\n",
      "Epoch 478/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6011 - acc: 0.7876 - val_loss: 0.6550 - val_acc: 0.7654\n",
      "Epoch 479/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5648 - acc: 0.8092 - val_loss: 0.6840 - val_acc: 0.7531\n",
      "Epoch 480/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6431 - acc: 0.7679 - val_loss: 0.8617 - val_acc: 0.6420\n",
      "Epoch 481/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6700 - acc: 0.7500 - val_loss: 0.6866 - val_acc: 0.7469\n",
      "Epoch 482/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6026 - acc: 0.7853 - val_loss: 0.7152 - val_acc: 0.7407\n",
      "Epoch 483/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6155 - acc: 0.7790 - val_loss: 0.6984 - val_acc: 0.7284\n",
      "Epoch 484/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6313 - acc: 0.7663 - val_loss: 0.6746 - val_acc: 0.7469\n",
      "Epoch 485/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5898 - acc: 0.7918 - val_loss: 0.6769 - val_acc: 0.7469\n",
      "Epoch 486/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5799 - acc: 0.7994 - val_loss: 0.7317 - val_acc: 0.7160\n",
      "Epoch 487/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5928 - acc: 0.7925 - val_loss: 0.6809 - val_acc: 0.7593\n",
      "Epoch 488/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.7606 - val_loss: 0.8273 - val_acc: 0.6852\n",
      "Epoch 489/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6371 - acc: 0.7666 - val_loss: 0.6696 - val_acc: 0.7531\n",
      "Epoch 490/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5721 - acc: 0.8067 - val_loss: 0.7160 - val_acc: 0.7346\n",
      "Epoch 491/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5846 - acc: 0.7975 - val_loss: 0.7677 - val_acc: 0.6975\n",
      "Epoch 492/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6308 - acc: 0.7695 - val_loss: 0.6831 - val_acc: 0.7346\n",
      "Epoch 493/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5762 - acc: 0.8016 - val_loss: 0.6576 - val_acc: 0.7654\n",
      "Epoch 494/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5762 - acc: 0.8018 - val_loss: 0.6735 - val_acc: 0.7593\n",
      "Epoch 495/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6904 - acc: 0.7385 - val_loss: 0.6913 - val_acc: 0.7531\n",
      "Epoch 496/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5957 - acc: 0.7919 - val_loss: 0.6739 - val_acc: 0.7469\n",
      "Epoch 497/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5677 - acc: 0.8076 - val_loss: 0.7147 - val_acc: 0.7346\n",
      "Epoch 498/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5971 - acc: 0.7902 - val_loss: 0.6651 - val_acc: 0.7593\n",
      "Epoch 499/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5710 - acc: 0.8057 - val_loss: 0.7551 - val_acc: 0.6914\n",
      "Epoch 500/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6662 - acc: 0.7509 - val_loss: 0.6687 - val_acc: 0.7531\n",
      "Epoch 501/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6146 - acc: 0.7774 - val_loss: 0.6589 - val_acc: 0.7469\n",
      "Epoch 502/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5690 - acc: 0.8074 - val_loss: 0.6414 - val_acc: 0.7654\n",
      "Epoch 503/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5681 - acc: 0.8063 - val_loss: 0.7207 - val_acc: 0.7099\n",
      "Epoch 504/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6744 - acc: 0.7492 - val_loss: 0.6884 - val_acc: 0.7531\n",
      "Epoch 505/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5907 - acc: 0.7931 - val_loss: 0.7018 - val_acc: 0.7222\n",
      "Epoch 506/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5790 - acc: 0.8000 - val_loss: 0.6636 - val_acc: 0.7654\n",
      "Epoch 507/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6608 - acc: 0.7521 - val_loss: 0.6581 - val_acc: 0.7716\n",
      "Epoch 508/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6017 - acc: 0.7856 - val_loss: 0.6538 - val_acc: 0.7284\n",
      "Epoch 509/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5626 - acc: 0.8115 - val_loss: 0.7034 - val_acc: 0.7346\n",
      "Epoch 510/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5915 - acc: 0.7928 - val_loss: 0.6654 - val_acc: 0.7469\n",
      "Epoch 511/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5666 - acc: 0.8078 - val_loss: 0.6556 - val_acc: 0.7593\n",
      "Epoch 512/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5819 - acc: 0.7991 - val_loss: 0.6949 - val_acc: 0.7160\n",
      "Epoch 513/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5844 - acc: 0.7974 - val_loss: 0.6542 - val_acc: 0.7778\n",
      "Epoch 514/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5800 - acc: 0.7984 - val_loss: 0.6860 - val_acc: 0.7407\n",
      "Epoch 515/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5835 - acc: 0.7977 - val_loss: 0.6583 - val_acc: 0.7778\n",
      "Epoch 516/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5835 - acc: 0.7983 - val_loss: 0.6923 - val_acc: 0.7346\n",
      "Epoch 517/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5889 - acc: 0.7941 - val_loss: 0.7159 - val_acc: 0.7407\n",
      "Epoch 518/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5885 - acc: 0.7952 - val_loss: 0.6764 - val_acc: 0.7407\n",
      "Epoch 519/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5703 - acc: 0.8053 - val_loss: 0.6858 - val_acc: 0.7593\n",
      "Epoch 520/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6067 - acc: 0.7832 - val_loss: 0.8304 - val_acc: 0.6543\n",
      "Epoch 521/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.6126 - acc: 0.7776 - val_loss: 0.6400 - val_acc: 0.7593\n",
      "Epoch 522/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6237 - acc: 0.7723 - val_loss: 0.6831 - val_acc: 0.7469\n",
      "Epoch 523/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5695 - acc: 0.8064 - val_loss: 0.6664 - val_acc: 0.7407\n",
      "Epoch 524/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5575 - acc: 0.8140 - val_loss: 0.7150 - val_acc: 0.7469\n",
      "Epoch 525/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6071 - acc: 0.7850 - val_loss: 0.6536 - val_acc: 0.7654\n",
      "Epoch 526/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5645 - acc: 0.8090 - val_loss: 0.7083 - val_acc: 0.7469\n",
      "Epoch 527/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6691 - acc: 0.7509 - val_loss: 0.6877 - val_acc: 0.7654\n",
      "Epoch 528/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6310 - acc: 0.7705 - val_loss: 0.6922 - val_acc: 0.7407\n",
      "Epoch 529/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5713 - acc: 0.8065 - val_loss: 0.6736 - val_acc: 0.7531\n",
      "Epoch 530/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5776 - acc: 0.8025 - val_loss: 0.6645 - val_acc: 0.7469\n",
      "Epoch 531/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6255 - acc: 0.7740 - val_loss: 0.6837 - val_acc: 0.7407\n",
      "Epoch 532/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6338 - acc: 0.7682 - val_loss: 0.7299 - val_acc: 0.7099\n",
      "Epoch 533/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5892 - acc: 0.7936 - val_loss: 0.7465 - val_acc: 0.7284\n",
      "Epoch 534/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6218 - acc: 0.7745 - val_loss: 0.6910 - val_acc: 0.7469\n",
      "Epoch 535/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5627 - acc: 0.8106 - val_loss: 0.6764 - val_acc: 0.7407\n",
      "Epoch 536/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5688 - acc: 0.8070 - val_loss: 0.6899 - val_acc: 0.7407\n",
      "Epoch 537/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5730 - acc: 0.8035 - val_loss: 0.7020 - val_acc: 0.7407\n",
      "Epoch 538/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5787 - acc: 0.8019 - val_loss: 0.7512 - val_acc: 0.7284\n",
      "Epoch 539/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6689 - acc: 0.7517 - val_loss: 0.6498 - val_acc: 0.7716\n",
      "Epoch 540/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6102 - acc: 0.7815 - val_loss: 0.6401 - val_acc: 0.7531\n",
      "Epoch 541/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5698 - acc: 0.8063 - val_loss: 0.6993 - val_acc: 0.7407\n",
      "Epoch 542/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5762 - acc: 0.8027 - val_loss: 0.6928 - val_acc: 0.7346\n",
      "Epoch 543/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5620 - acc: 0.8108 - val_loss: 0.6749 - val_acc: 0.7654\n",
      "Epoch 544/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5844 - acc: 0.7960 - val_loss: 0.6816 - val_acc: 0.7284\n",
      "Epoch 545/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5938 - acc: 0.7934 - val_loss: 0.6874 - val_acc: 0.7469\n",
      "Epoch 546/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5681 - acc: 0.8080 - val_loss: 0.6523 - val_acc: 0.7593\n",
      "Epoch 547/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5759 - acc: 0.8028 - val_loss: 0.6661 - val_acc: 0.7593\n",
      "Epoch 548/1000\n",
      "161135/161135 [==============================] - 2s 9us/step - loss: 0.5788 - acc: 0.8008 - val_loss: 0.6714 - val_acc: 0.7531\n",
      "Epoch 549/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5805 - acc: 0.7996 - val_loss: 0.6678 - val_acc: 0.7469\n",
      "Epoch 550/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5677 - acc: 0.8071 - val_loss: 0.6975 - val_acc: 0.7222\n",
      "Epoch 551/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5854 - acc: 0.7972 - val_loss: 0.6623 - val_acc: 0.7778\n",
      "Epoch 552/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5845 - acc: 0.7974 - val_loss: 0.6596 - val_acc: 0.7346\n",
      "Epoch 553/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5721 - acc: 0.8037 - val_loss: 0.6759 - val_acc: 0.7407\n",
      "Epoch 554/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6967 - acc: 0.7385 - val_loss: 0.6725 - val_acc: 0.7778\n",
      "Epoch 555/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5975 - acc: 0.7915 - val_loss: 0.6737 - val_acc: 0.7654\n",
      "Epoch 556/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5654 - acc: 0.8090 - val_loss: 0.6827 - val_acc: 0.7593\n",
      "Epoch 557/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5829 - acc: 0.7994 - val_loss: 0.7183 - val_acc: 0.7407\n",
      "Epoch 558/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6351 - acc: 0.7685 - val_loss: 0.6626 - val_acc: 0.7407\n",
      "Epoch 559/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5874 - acc: 0.7962 - val_loss: 0.6547 - val_acc: 0.7346\n",
      "Epoch 560/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5578 - acc: 0.8148 - val_loss: 0.6682 - val_acc: 0.7469\n",
      "Epoch 561/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5639 - acc: 0.8105 - val_loss: 0.6686 - val_acc: 0.7654\n",
      "Epoch 562/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6020 - acc: 0.7884 - val_loss: 0.6766 - val_acc: 0.7222\n",
      "Epoch 563/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5667 - acc: 0.8088 - val_loss: 0.6779 - val_acc: 0.7407\n",
      "Epoch 564/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.7625 - val_loss: 0.7914 - val_acc: 0.6852\n",
      "Epoch 565/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6419 - acc: 0.7653 - val_loss: 0.6879 - val_acc: 0.7469\n",
      "Epoch 566/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5655 - acc: 0.8103 - val_loss: 0.6842 - val_acc: 0.7346\n",
      "Epoch 567/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5697 - acc: 0.8071 - val_loss: 0.7091 - val_acc: 0.7160\n",
      "Epoch 568/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5772 - acc: 0.8030 - val_loss: 0.7224 - val_acc: 0.7469\n",
      "Epoch 569/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.6319 - acc: 0.7699 - val_loss: 0.6766 - val_acc: 0.7531\n",
      "Epoch 570/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5920 - acc: 0.7917 - val_loss: 0.6920 - val_acc: 0.7407\n",
      "Epoch 571/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5674 - acc: 0.8085 - val_loss: 0.6740 - val_acc: 0.7346\n",
      "Epoch 572/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5772 - acc: 0.8017 - val_loss: 0.6861 - val_acc: 0.7407\n",
      "Epoch 573/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5718 - acc: 0.8040 - val_loss: 0.6611 - val_acc: 0.7654\n",
      "Epoch 574/1000\n",
      "161135/161135 [==============================] - 2s 10us/step - loss: 0.5670 - acc: 0.8088 - val_loss: 0.6747 - val_acc: 0.7593\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model_bert.fit(all_embeddings, y, epochs=1000, batch_size=10000, \n",
    "                             validation_split = 0.001, callbacks=cb_list)\n",
    "    model_bert.save_weights('../model/bert_logistic/model_bert_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000\n",
    "batches = math.ceil(all_embeddings_test.shape[0] / bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Batch 1\n",
      "Predicting Batch 2\n",
      "Predicting Batch 3\n",
      "Predicting Batch 4\n",
      "Predicting Batch 5\n",
      "Predicting Batch 6\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_probs = []\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    model_bert.load_weights('../model/bert_logistic/model_bert_weights.h5')\n",
    "\n",
    "    for i in range(1,batches+1):\n",
    "        print(\"Predicting Batch\",i)\n",
    "        new_text_pr = all_embeddings_test[(i-1)*bs:i*bs]\n",
    "        preds = model_bert.predict(new_text_pr)\n",
    "        all_probs.append(preds)\n",
    "        preds = encoder.inverse_transform(np.argmax(preds,axis=1))\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate(all_preds, axis=0)\n",
    "results_probs = np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/bert_logistic/test_results.tsv\", results_probs, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/bert_logistic/test_predictions.tsv\", results, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.719953130231001\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",sum(results==y_test)/results.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
