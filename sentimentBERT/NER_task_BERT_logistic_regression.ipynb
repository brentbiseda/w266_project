{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Hugging Face Transformers\n",
    "\n",
    "https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install keras\n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import transformers as ppb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-cased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/NER/train.tsv\", sep=\"\\t\", header=None)\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "test_df = pd.read_csv(\"../datasets/NER/test.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = test_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long sentences to 128 tokens\n",
    "X = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y = np.array(df[1])\n",
    "y = ['missing' if x is np.nan else x for x in y]\n",
    "del df\n",
    "\n",
    "X_test = test_df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128)))\n",
    "y_test = np.array(test_df[1])\n",
    "y_test = ['missing' if x is np.nan else x for x in y_test]\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding of y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "y = encoder.transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# One hot Encoding of y test\n",
    "y_oh = encoder.transform(y_test)\n",
    "y_oh = to_categorical(y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(tokenizedBatch):\n",
    "    max_len = 0\n",
    "    for i in tokenizedBatch.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenizedBatch.values])\n",
    "    \n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    input_ids = torch.tensor(padded).to(torch.long)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for Batch: 1 of 4\n",
      "Generating Embeddings for Batch: 2 of 4\n",
      "Generating Embeddings for Batch: 3 of 4\n",
      "Generating Embeddings for Batch: 4 of 4\n",
      "Generating Test Embeddings for Batch: 1 of 2\n",
      "Generating Test Embeddings for Batch: 2 of 2\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000\n",
    "all_embeddings = []\n",
    "all_embeddings_test = []\n",
    "\n",
    "# Process Training Set Embeddings\n",
    "batches = math.ceil(X.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings.append(batchEmbeddings)\n",
    "\n",
    "# Process Test Set Embeddings\n",
    "batches = math.ceil(X_test.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Test Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X_test[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings_test.append(batchEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "all_embeddings_test = np.concatenate(all_embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../binary/bert_NER_embeddings_twitter.npy', all_embeddings)\n",
    "np.save('../binary/y_NER_twitter.npy', y)\n",
    "np.save('../binary/bert_NER_embeddings_test_twitter.npy', all_embeddings_test)\n",
    "np.save('../binary/y_test_NER_twitter.npy', y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.optimizers import adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('../binary/bert_NER_embeddings_twitter.npy')\n",
    "y = np.load('../binary/y_NER_twitter.npy')\n",
    "all_embeddings_test = np.load('../binary/bert_NER_embeddings_test_twitter.npy')\n",
    "y_oh = np.load('../binary/y_test_NER_twitter.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisedab\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#sgd = sgd(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optim = adam(lr=0.0003, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    embedding = Input(shape=(768,), dtype=\"float\")\n",
    "    dense1 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)\n",
    "    dense2 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense1)\n",
    "    dense3 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense2)\n",
    "    dense4 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense3)\n",
    "    dense5 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense4)\n",
    "    dense6 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense5)\n",
    "    dense7 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense6)\n",
    "    dense8 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense7)\n",
    "    dense9 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense8)\n",
    "    dense10 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense9)\n",
    "    pred = Dense(4, activation='sigmoid')(dense9)\n",
    "    model = Model(inputs=[embedding], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'], )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              769000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 8,781,004\n",
      "Trainable params: 8,781,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=15)\n",
    "cb_list = [es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisedab\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15619 samples, validate on 16 samples\n",
      "Epoch 1/1000\n",
      "15619/15619 [==============================] - 3s 186us/step - loss: 10.1370 - acc: 0.7936 - val_loss: 9.8023 - val_acc: 0.6250\n",
      "Epoch 2/1000\n",
      "15619/15619 [==============================] - 0s 13us/step - loss: 9.3882 - acc: 0.9365 - val_loss: 9.9416 - val_acc: 0.6250\n",
      "Epoch 3/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 8.7776 - acc: 0.9365 - val_loss: 11.0300 - val_acc: 0.6250\n",
      "Epoch 4/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 8.6819 - acc: 0.9365 - val_loss: 9.5177 - val_acc: 0.6250\n",
      "Epoch 5/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 8.3510 - acc: 0.9365 - val_loss: 8.8557 - val_acc: 0.6250\n",
      "Epoch 6/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 8.1887 - acc: 0.9365 - val_loss: 8.9740 - val_acc: 0.6250\n",
      "Epoch 7/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 7.9339 - acc: 0.9365 - val_loss: 9.1788 - val_acc: 0.6250\n",
      "Epoch 8/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 7.7570 - acc: 0.9365 - val_loss: 8.8529 - val_acc: 0.6250\n",
      "Epoch 9/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 7.5453 - acc: 0.9365 - val_loss: 8.3614 - val_acc: 0.6250\n",
      "Epoch 10/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 7.3565 - acc: 0.9365 - val_loss: 8.0823 - val_acc: 0.6250\n",
      "Epoch 11/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 7.1795 - acc: 0.9365 - val_loss: 8.0168 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.9892 - acc: 0.9365 - val_loss: 8.0567 - val_acc: 0.6250\n",
      "Epoch 13/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 6.8169 - acc: 0.9365 - val_loss: 7.9424 - val_acc: 0.6250\n",
      "Epoch 14/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 6.6456 - acc: 0.9365 - val_loss: 7.6373 - val_acc: 0.6250\n",
      "Epoch 15/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 6.4735 - acc: 0.9365 - val_loss: 7.3568 - val_acc: 0.6250\n",
      "Epoch 16/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 6.3120 - acc: 0.9365 - val_loss: 7.2121 - val_acc: 0.6250\n",
      "Epoch 17/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 6.1486 - acc: 0.9365 - val_loss: 7.1675 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 5.9921 - acc: 0.9365 - val_loss: 7.0702 - val_acc: 0.6250\n",
      "Epoch 19/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 5.8405 - acc: 0.9365 - val_loss: 6.8402 - val_acc: 0.6250\n",
      "Epoch 20/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 5.6895 - acc: 0.9365 - val_loss: 6.6095 - val_acc: 0.6250\n",
      "Epoch 21/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.5455 - acc: 0.9365 - val_loss: 6.4894 - val_acc: 0.6250\n",
      "Epoch 22/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 5.4028 - acc: 0.9365 - val_loss: 6.4350 - val_acc: 0.6250\n",
      "Epoch 23/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 5.2647 - acc: 0.9365 - val_loss: 6.3147 - val_acc: 0.6250\n",
      "Epoch 24/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 5.1295 - acc: 0.9365 - val_loss: 6.1105 - val_acc: 0.6250\n",
      "Epoch 25/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 4.9980 - acc: 0.9365 - val_loss: 5.9791 - val_acc: 0.6250\n",
      "Epoch 26/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.8695 - acc: 0.9365 - val_loss: 5.9319 - val_acc: 0.6250\n",
      "Epoch 27/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.7441 - acc: 0.9365 - val_loss: 5.8047 - val_acc: 0.6250\n",
      "Epoch 28/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 4.6218 - acc: 0.9365 - val_loss: 5.6327 - val_acc: 0.6250\n",
      "Epoch 29/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.5032 - acc: 0.9365 - val_loss: 5.5589 - val_acc: 0.6250\n",
      "Epoch 30/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.3876 - acc: 0.9365 - val_loss: 5.4639 - val_acc: 0.6250\n",
      "Epoch 31/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 4.2750 - acc: 0.9365 - val_loss: 5.2974 - val_acc: 0.6250\n",
      "Epoch 32/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 4.1665 - acc: 0.9365 - val_loss: 5.2435 - val_acc: 0.6250\n",
      "Epoch 33/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 4.0603 - acc: 0.9365 - val_loss: 5.0650 - val_acc: 0.6250\n",
      "Epoch 34/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.9578 - acc: 0.9365 - val_loss: 5.0267 - val_acc: 0.6250\n",
      "Epoch 35/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.8585 - acc: 0.9365 - val_loss: 4.8944 - val_acc: 0.6250\n",
      "Epoch 36/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.7609 - acc: 0.9365 - val_loss: 4.7853 - val_acc: 0.6250\n",
      "Epoch 37/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.6664 - acc: 0.9365 - val_loss: 4.6702 - val_acc: 0.6250\n",
      "Epoch 38/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.5748 - acc: 0.9365 - val_loss: 4.5950 - val_acc: 0.6250\n",
      "Epoch 39/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.4858 - acc: 0.9365 - val_loss: 4.4631 - val_acc: 0.6250\n",
      "Epoch 40/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.3999 - acc: 0.9365 - val_loss: 4.4430 - val_acc: 0.6250\n",
      "Epoch 41/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 3.3160 - acc: 0.9365 - val_loss: 4.2993 - val_acc: 0.6250\n",
      "Epoch 42/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.2343 - acc: 0.9365 - val_loss: 4.1353 - val_acc: 0.6250\n",
      "Epoch 43/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.1554 - acc: 0.9365 - val_loss: 4.0380 - val_acc: 0.6250\n",
      "Epoch 44/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.0784 - acc: 0.9365 - val_loss: 3.8924 - val_acc: 0.6250\n",
      "Epoch 45/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 3.0043 - acc: 0.9365 - val_loss: 3.8680 - val_acc: 0.6250\n",
      "Epoch 46/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.9329 - acc: 0.9365 - val_loss: 3.9709 - val_acc: 0.6250\n",
      "Epoch 47/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.8687 - acc: 0.9365 - val_loss: 3.5718 - val_acc: 0.6250\n",
      "Epoch 48/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.8011 - acc: 0.9365 - val_loss: 3.7039 - val_acc: 0.6250\n",
      "Epoch 49/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.7341 - acc: 0.9365 - val_loss: 3.6428 - val_acc: 0.6250\n",
      "Epoch 50/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.6695 - acc: 0.9365 - val_loss: 3.4768 - val_acc: 0.6250\n",
      "Epoch 51/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.6075 - acc: 0.9365 - val_loss: 3.5641 - val_acc: 0.6250\n",
      "Epoch 52/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.5482 - acc: 0.9365 - val_loss: 3.3631 - val_acc: 0.6250\n",
      "Epoch 53/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.4901 - acc: 0.9365 - val_loss: 3.4776 - val_acc: 0.6250\n",
      "Epoch 54/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.4354 - acc: 0.9365 - val_loss: 3.2452 - val_acc: 0.6250\n",
      "Epoch 55/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.3797 - acc: 0.9365 - val_loss: 3.2990 - val_acc: 0.6250\n",
      "Epoch 56/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.3259 - acc: 0.9365 - val_loss: 3.1478 - val_acc: 0.6250\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 11us/step - loss: 2.2755 - acc: 0.9365 - val_loss: 3.1613 - val_acc: 0.6250\n",
      "Epoch 58/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.2259 - acc: 0.9365 - val_loss: 3.2575 - val_acc: 0.6250\n",
      "Epoch 59/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.1799 - acc: 0.9365 - val_loss: 2.9710 - val_acc: 0.6250\n",
      "Epoch 60/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 2.1330 - acc: 0.9365 - val_loss: 3.0456 - val_acc: 0.6250\n",
      "Epoch 61/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 2.0852 - acc: 0.9365 - val_loss: 3.0539 - val_acc: 0.6250\n",
      "Epoch 62/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 2.0411 - acc: 0.9365 - val_loss: 2.8634 - val_acc: 0.6250\n",
      "Epoch 63/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 1.9981 - acc: 0.9365 - val_loss: 2.9409 - val_acc: 0.6250\n",
      "Epoch 64/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.9589 - acc: 0.9365 - val_loss: 3.1067 - val_acc: 0.6250\n",
      "Epoch 65/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.9241 - acc: 0.9365 - val_loss: 2.6688 - val_acc: 0.6250\n",
      "Epoch 66/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.8925 - acc: 0.9365 - val_loss: 2.8811 - val_acc: 0.6250\n",
      "Epoch 67/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.8498 - acc: 0.9365 - val_loss: 2.9343 - val_acc: 0.6250\n",
      "Epoch 68/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.8136 - acc: 0.9365 - val_loss: 2.6293 - val_acc: 0.6250\n",
      "Epoch 69/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 1.7764 - acc: 0.9364 - val_loss: 2.8518 - val_acc: 0.6250\n",
      "Epoch 70/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.7439 - acc: 0.9364 - val_loss: 2.6237 - val_acc: 0.6250\n",
      "Epoch 71/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.7067 - acc: 0.9365 - val_loss: 2.6387 - val_acc: 0.6250\n",
      "Epoch 72/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.6732 - acc: 0.9364 - val_loss: 2.6524 - val_acc: 0.6250\n",
      "Epoch 73/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.6417 - acc: 0.9367 - val_loss: 2.4849 - val_acc: 0.6250\n",
      "Epoch 74/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.6099 - acc: 0.9367 - val_loss: 2.6408 - val_acc: 0.6250\n",
      "Epoch 75/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.5819 - acc: 0.9366 - val_loss: 2.4067 - val_acc: 0.6250\n",
      "Epoch 76/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.5519 - acc: 0.9366 - val_loss: 2.5998 - val_acc: 0.6250\n",
      "Epoch 77/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.5253 - acc: 0.9366 - val_loss: 2.3528 - val_acc: 0.6250\n",
      "Epoch 78/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4965 - acc: 0.9367 - val_loss: 2.5073 - val_acc: 0.6250\n",
      "Epoch 79/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4703 - acc: 0.9366 - val_loss: 2.2866 - val_acc: 0.6250\n",
      "Epoch 80/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4438 - acc: 0.9366 - val_loss: 2.4065 - val_acc: 0.6250\n",
      "Epoch 81/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.4194 - acc: 0.9368 - val_loss: 2.3303 - val_acc: 0.6250\n",
      "Epoch 82/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.3939 - acc: 0.9366 - val_loss: 2.2212 - val_acc: 0.6250\n",
      "Epoch 83/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.3704 - acc: 0.9367 - val_loss: 2.4043 - val_acc: 0.6250\n",
      "Epoch 84/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.3481 - acc: 0.9367 - val_loss: 2.2209 - val_acc: 0.6250\n",
      "Epoch 85/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.3268 - acc: 0.9367 - val_loss: 2.1888 - val_acc: 0.6250\n",
      "Epoch 86/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.3031 - acc: 0.9367 - val_loss: 2.3655 - val_acc: 0.6250\n",
      "Epoch 87/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2834 - acc: 0.9368 - val_loss: 2.3142 - val_acc: 0.6250\n",
      "Epoch 88/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2603 - acc: 0.9367 - val_loss: 2.1295 - val_acc: 0.6250\n",
      "Epoch 89/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2419 - acc: 0.9367 - val_loss: 2.1309 - val_acc: 0.6250\n",
      "Epoch 90/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.2203 - acc: 0.9369 - val_loss: 2.4455 - val_acc: 0.6250\n",
      "Epoch 91/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.2121 - acc: 0.9373 - val_loss: 2.3006 - val_acc: 0.6250\n",
      "Epoch 92/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1844 - acc: 0.9367 - val_loss: 1.9726 - val_acc: 0.6250\n",
      "Epoch 93/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1909 - acc: 0.9394 - val_loss: 2.0753 - val_acc: 0.6250\n",
      "Epoch 94/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 1.1580 - acc: 0.9382 - val_loss: 2.6513 - val_acc: 0.6250\n",
      "Epoch 95/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.1659 - acc: 0.9366 - val_loss: 1.8171 - val_acc: 0.6250\n",
      "Epoch 96/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.1530 - acc: 0.9410 - val_loss: 2.6074 - val_acc: 0.6250\n",
      "Epoch 97/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 1.1336 - acc: 0.9366 - val_loss: 1.8423 - val_acc: 0.6250\n",
      "Epoch 98/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 1.1126 - acc: 0.9384 - val_loss: 2.2349 - val_acc: 0.6250\n",
      "Epoch 99/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0930 - acc: 0.9366 - val_loss: 2.1507 - val_acc: 0.6250\n",
      "Epoch 100/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0704 - acc: 0.9371 - val_loss: 1.8279 - val_acc: 0.6250\n",
      "Epoch 101/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0607 - acc: 0.9373 - val_loss: 2.2922 - val_acc: 0.6250\n",
      "Epoch 102/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 1.0479 - acc: 0.9367 - val_loss: 1.8892 - val_acc: 0.6250\n",
      "Epoch 103/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0287 - acc: 0.9377 - val_loss: 1.9491 - val_acc: 0.6250\n",
      "Epoch 104/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 1.0129 - acc: 0.9369 - val_loss: 2.0868 - val_acc: 0.6250\n",
      "Epoch 105/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 1.0007 - acc: 0.9372 - val_loss: 1.7907 - val_acc: 0.6250\n",
      "Epoch 106/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.9880 - acc: 0.9392 - val_loss: 2.0332 - val_acc: 0.6250\n",
      "Epoch 107/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9740 - acc: 0.9369 - val_loss: 1.8003 - val_acc: 0.6250\n",
      "Epoch 108/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9621 - acc: 0.9401 - val_loss: 1.9885 - val_acc: 0.6250\n",
      "Epoch 109/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9497 - acc: 0.9369 - val_loss: 1.8480 - val_acc: 0.6250\n",
      "Epoch 110/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.9371 - acc: 0.9403 - val_loss: 1.9192 - val_acc: 0.6250\n",
      "Epoch 111/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.9254 - acc: 0.9374 - val_loss: 1.8477 - val_acc: 0.6250\n",
      "Epoch 112/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9143 - acc: 0.9404 - val_loss: 1.9364 - val_acc: 0.6250\n",
      "Epoch 113/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.9034 - acc: 0.9381 - val_loss: 1.8073 - val_acc: 0.6250\n",
      "Epoch 114/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8930 - acc: 0.9425 - val_loss: 2.0227 - val_acc: 0.6250\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8834 - acc: 0.9385 - val_loss: 1.7478 - val_acc: 0.6250\n",
      "Epoch 116/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8773 - acc: 0.9429 - val_loss: 1.9170 - val_acc: 0.6250\n",
      "Epoch 117/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.8652 - acc: 0.9421 - val_loss: 2.1015 - val_acc: 0.6250\n",
      "Epoch 118/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8583 - acc: 0.9396 - val_loss: 1.8315 - val_acc: 0.6250\n",
      "Epoch 119/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.8440 - acc: 0.9437 - val_loss: 1.9570 - val_acc: 0.6250\n",
      "Epoch 120/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8340 - acc: 0.9439 - val_loss: 2.0458 - val_acc: 0.6250\n",
      "Epoch 121/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8258 - acc: 0.9412 - val_loss: 1.8250 - val_acc: 0.6250\n",
      "Epoch 122/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8186 - acc: 0.9447 - val_loss: 1.7786 - val_acc: 0.6250\n",
      "Epoch 123/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.8104 - acc: 0.9474 - val_loss: 1.8491 - val_acc: 0.6250\n",
      "Epoch 124/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7976 - acc: 0.9473 - val_loss: 1.8814 - val_acc: 0.6250\n",
      "Epoch 125/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.7900 - acc: 0.9455 - val_loss: 1.9965 - val_acc: 0.6250\n",
      "Epoch 126/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7897 - acc: 0.9461 - val_loss: 2.4820 - val_acc: 0.6250\n",
      "Epoch 127/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.8165 - acc: 0.9402 - val_loss: 2.0550 - val_acc: 0.6250\n",
      "Epoch 128/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7768 - acc: 0.9437 - val_loss: 1.6236 - val_acc: 0.6250\n",
      "Epoch 129/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7704 - acc: 0.9491 - val_loss: 2.0029 - val_acc: 0.6250\n",
      "Epoch 130/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7640 - acc: 0.9428 - val_loss: 1.5973 - val_acc: 0.6250\n",
      "Epoch 131/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7553 - acc: 0.9459 - val_loss: 1.8106 - val_acc: 0.6250\n",
      "Epoch 132/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7443 - acc: 0.9479 - val_loss: 2.1088 - val_acc: 0.6250\n",
      "Epoch 133/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7418 - acc: 0.9424 - val_loss: 1.6968 - val_acc: 0.6250\n",
      "Epoch 134/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7317 - acc: 0.9485 - val_loss: 1.7223 - val_acc: 0.6250\n",
      "Epoch 135/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7203 - acc: 0.9501 - val_loss: 1.9014 - val_acc: 0.6250\n",
      "Epoch 136/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7171 - acc: 0.9467 - val_loss: 1.6838 - val_acc: 0.6250\n",
      "Epoch 137/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7097 - acc: 0.9491 - val_loss: 1.7319 - val_acc: 0.6250\n",
      "Epoch 138/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7010 - acc: 0.9522 - val_loss: 2.0632 - val_acc: 0.6250\n",
      "Epoch 139/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.7009 - acc: 0.9483 - val_loss: 2.2465 - val_acc: 0.6250\n",
      "Epoch 140/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7035 - acc: 0.9462 - val_loss: 1.8812 - val_acc: 0.6250\n",
      "Epoch 141/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6825 - acc: 0.9511 - val_loss: 1.6583 - val_acc: 0.6250\n",
      "Epoch 142/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6828 - acc: 0.9505 - val_loss: 1.6199 - val_acc: 0.6250\n",
      "Epoch 143/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6750 - acc: 0.9519 - val_loss: 1.7900 - val_acc: 0.6250\n",
      "Epoch 144/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6637 - acc: 0.9533 - val_loss: 2.0130 - val_acc: 0.6250\n",
      "Epoch 145/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6644 - acc: 0.9496 - val_loss: 2.2629 - val_acc: 0.6250\n",
      "Epoch 146/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6668 - acc: 0.9485 - val_loss: 1.9145 - val_acc: 0.6250\n",
      "Epoch 147/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6483 - acc: 0.9518 - val_loss: 1.5816 - val_acc: 0.6250\n",
      "Epoch 148/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6630 - acc: 0.9479 - val_loss: 1.5306 - val_acc: 0.5625\n",
      "Epoch 149/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6588 - acc: 0.9493 - val_loss: 1.8248 - val_acc: 0.6250\n",
      "Epoch 150/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6399 - acc: 0.9520 - val_loss: 2.1075 - val_acc: 0.6250\n",
      "Epoch 151/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6426 - acc: 0.9479 - val_loss: 1.5193 - val_acc: 0.6250\n",
      "Epoch 152/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6413 - acc: 0.9488 - val_loss: 1.7544 - val_acc: 0.6250\n",
      "Epoch 153/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6293 - acc: 0.9517 - val_loss: 2.4244 - val_acc: 0.6250\n",
      "Epoch 154/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6481 - acc: 0.9447 - val_loss: 1.4708 - val_acc: 0.6250\n",
      "Epoch 155/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6475 - acc: 0.9442 - val_loss: 2.3025 - val_acc: 0.6250\n",
      "Epoch 156/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6429 - acc: 0.9433 - val_loss: 1.4613 - val_acc: 0.6250\n",
      "Epoch 157/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6291 - acc: 0.9479 - val_loss: 2.0322 - val_acc: 0.6250\n",
      "Epoch 158/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6176 - acc: 0.9480 - val_loss: 1.3950 - val_acc: 0.6250\n",
      "Epoch 159/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6176 - acc: 0.9490 - val_loss: 1.9569 - val_acc: 0.6250\n",
      "Epoch 160/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6092 - acc: 0.9453 - val_loss: 1.4425 - val_acc: 0.6250\n",
      "Epoch 161/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.6035 - acc: 0.9517 - val_loss: 1.9978 - val_acc: 0.6250\n",
      "Epoch 162/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5966 - acc: 0.9476 - val_loss: 1.4529 - val_acc: 0.6250\n",
      "Epoch 163/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5958 - acc: 0.9520 - val_loss: 2.0255 - val_acc: 0.6250\n",
      "Epoch 164/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5916 - acc: 0.9475 - val_loss: 1.4784 - val_acc: 0.6250\n",
      "Epoch 165/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5830 - acc: 0.9530 - val_loss: 1.9760 - val_acc: 0.6250\n",
      "Epoch 166/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5788 - acc: 0.9495 - val_loss: 1.5073 - val_acc: 0.6250\n",
      "Epoch 167/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5729 - acc: 0.9538 - val_loss: 1.9797 - val_acc: 0.6250\n",
      "Epoch 168/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5694 - acc: 0.9508 - val_loss: 1.5155 - val_acc: 0.6250\n",
      "Epoch 169/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5669 - acc: 0.9535 - val_loss: 1.9134 - val_acc: 0.6250\n",
      "Epoch 170/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5604 - acc: 0.9531 - val_loss: 1.6895 - val_acc: 0.6250\n",
      "Epoch 171/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.5528 - acc: 0.9542 - val_loss: 1.6620 - val_acc: 0.6250\n",
      "Epoch 172/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5489 - acc: 0.9551 - val_loss: 1.9878 - val_acc: 0.6250\n",
      "Epoch 173/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5495 - acc: 0.9533 - val_loss: 1.6415 - val_acc: 0.6250\n",
      "Epoch 174/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5424 - acc: 0.9545 - val_loss: 1.6889 - val_acc: 0.6250\n",
      "Epoch 175/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5361 - acc: 0.9557 - val_loss: 1.9353 - val_acc: 0.6250\n",
      "Epoch 176/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5354 - acc: 0.9547 - val_loss: 1.7161 - val_acc: 0.6250\n",
      "Epoch 177/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5309 - acc: 0.9555 - val_loss: 1.5757 - val_acc: 0.6250\n",
      "Epoch 178/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5410 - acc: 0.9527 - val_loss: 1.6856 - val_acc: 0.6250\n",
      "Epoch 179/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5255 - acc: 0.9553 - val_loss: 2.0254 - val_acc: 0.6250\n",
      "Epoch 180/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5255 - acc: 0.9543 - val_loss: 1.6996 - val_acc: 0.6250\n",
      "Epoch 181/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5178 - acc: 0.9556 - val_loss: 1.6699 - val_acc: 0.6250\n",
      "Epoch 182/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5153 - acc: 0.9570 - val_loss: 1.9751 - val_acc: 0.6250\n",
      "Epoch 183/1000\n",
      "15619/15619 [==============================] - 0s 12us/step - loss: 0.5119 - acc: 0.9563 - val_loss: 2.1292 - val_acc: 0.6250\n",
      "Epoch 184/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5167 - acc: 0.9541 - val_loss: 2.0388 - val_acc: 0.6250\n",
      "Epoch 185/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5092 - acc: 0.9558 - val_loss: 1.8677 - val_acc: 0.6250\n",
      "Epoch 186/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5005 - acc: 0.9562 - val_loss: 1.6994 - val_acc: 0.6250\n",
      "Epoch 187/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.4981 - acc: 0.9564 - val_loss: 1.6393 - val_acc: 0.6250\n",
      "Epoch 188/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5007 - acc: 0.9553 - val_loss: 1.5037 - val_acc: 0.5625\n",
      "Epoch 189/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5875 - acc: 0.9428 - val_loss: 1.8583 - val_acc: 0.6250\n",
      "Epoch 190/1000\n",
      "15619/15619 [==============================] - 0s 9us/step - loss: 0.5591 - acc: 0.9335 - val_loss: 3.3750 - val_acc: 0.6250\n",
      "Epoch 191/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.7357 - acc: 0.9365 - val_loss: 2.3988 - val_acc: 0.6250\n",
      "Epoch 192/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5915 - acc: 0.9394 - val_loss: 1.1892 - val_acc: 0.6250\n",
      "Epoch 193/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.6474 - acc: 0.9439 - val_loss: 1.5218 - val_acc: 0.6250\n",
      "Epoch 194/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5431 - acc: 0.9399 - val_loss: 2.4403 - val_acc: 0.6250\n",
      "Epoch 195/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5980 - acc: 0.9369 - val_loss: 1.9447 - val_acc: 0.6250\n",
      "Epoch 196/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5565 - acc: 0.9387 - val_loss: 1.3876 - val_acc: 0.6250\n",
      "Epoch 197/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5619 - acc: 0.9418 - val_loss: 1.3638 - val_acc: 0.6250\n",
      "Epoch 198/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5369 - acc: 0.9405 - val_loss: 1.6923 - val_acc: 0.6250\n",
      "Epoch 199/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5349 - acc: 0.9382 - val_loss: 1.8670 - val_acc: 0.6250\n",
      "Epoch 200/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5374 - acc: 0.9381 - val_loss: 1.6104 - val_acc: 0.6250\n",
      "Epoch 201/1000\n",
      "15619/15619 [==============================] - 0s 11us/step - loss: 0.5234 - acc: 0.9414 - val_loss: 1.3958 - val_acc: 0.6250\n",
      "Epoch 202/1000\n",
      "15619/15619 [==============================] - 0s 10us/step - loss: 0.5208 - acc: 0.9449 - val_loss: 1.4862 - val_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    #tf.compat.v1.keras.backend.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model_bert.fit(all_embeddings, y, epochs=1000, batch_size=10000, \n",
    "                             validation_split = 0.001, callbacks=cb_list)\n",
    "    model_bert.save_weights('../model/bert_ner_logistic_twitter/model_bert_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#acc = history.history['acc']\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000\n",
    "batches = math.ceil(all_embeddings_test.shape[0] / bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Batch 1\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_probs = []\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    model_bert.load_weights('../model/bert_ner_logistic_twitter/model_bert_weights.h5')\n",
    "\n",
    "    for i in range(1,batches+1):\n",
    "        print(\"Predicting Batch\",i)\n",
    "        new_text_pr = all_embeddings_test[(i-1)*bs:i*bs]\n",
    "        preds = model_bert.predict(new_text_pr)\n",
    "        all_probs.append(preds)\n",
    "        preds = encoder.inverse_transform(np.argmax(preds,axis=1))\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate(all_preds, axis=0)\n",
    "results_probs = np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/NER/bert_logistic_twitter/test_results.tsv\", results_probs, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/NER/bert_logistic_twitter/test_predictions.tsv\", results, delimiter=\"\\t\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9481509013194573\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",sum(results==y_test)/results.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
